{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 00:35:53.699046: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-16 00:35:53.844050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-16 00:35:53.848329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-16 00:35:53.848344: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-16 00:35:54.867277: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 00:35:54.867338: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 00:35:54.867345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of jets to train on:\n",
      "141329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/physics/phujdj/DeepLearningParticlePhysics/hffrag.py:136: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = numpy.where(mask, pts, pts * numpy.sinh(etas))\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from DeepSetNeuralNetArchitecture import Mean_Squared_Error\n",
    "from keras import regularizers\n",
    "from Sum import Sum\n",
    "import matplotlib.pyplot as plt\n",
    "from hffrag import fixedbinning\n",
    "from hffrag import binneddensity\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the associated tracks for each jet\n",
    "def Match_Tracks(jets, tracks):\n",
    "    \"\"\"Used to determine if a set of tracks belong to a particular set of jets\"\"\"\n",
    "\n",
    "    jet_eta = jets[\"AnalysisAntiKt4TruthJets_eta\"]\n",
    "    jet_phi = jets[\"AnalysisAntiKt4TruthJets_phi\"] \n",
    "\n",
    "    tracks_eta = tracks[\"AnalysisTracks_eta\"]\n",
    "    tracks_phi = tracks[\"AnalysisTracks_phi\"]\n",
    "\n",
    "    delta_etas = jet_eta - tracks_eta\n",
    "    delta_phis = np.abs(jet_phi - tracks_phi)\n",
    "\n",
    "    # Map the phis from a cyclical period onto a linear relation\n",
    "    ak.where(delta_phis > np.pi, delta_phis - np.pi, delta_phis)\n",
    "\n",
    "    # Returns a list of true and false, determining which tracks belong to those jets.\n",
    "    return np.sqrt(delta_phis**2 + delta_etas**2) < 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_eta_phi_2_px_py_pz_tracks(pt_eta_phi, MASKVAL=-999):\n",
    "    \"\"\"Converts the cylindrical polar coordinates to cartesian coordinates for jets\"\"\"\n",
    "\n",
    "    # Seperate the pts, etas and phis\n",
    "    pts = pt_eta_phi[:, :, 0:1]\n",
    "    etas = pt_eta_phi[:, :, 1:2]\n",
    "    phis = pt_eta_phi[:, :, 2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    # Transforms only the non masked values from cylindrical to cartesian coordinates. Mask values are left unchanged.\n",
    "    mask1 = pts == MASKVAL \n",
    "    mask2 = phis == MASKVAL\n",
    "    mask3 = etas == MASKVAL\n",
    "    pxs = np.where(mask1 | mask2, pts, pts * np.cos(phis)) \n",
    "    pys = np.where(mask1 | mask2, pts, pts * np.sin(phis))\n",
    "    pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector in cartesian coordinates\n",
    "    return np.concatenate([pxs, pys, pzs], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_eta_phi_2_px_py_pz_tracks(pt_eta_phi, MASKVAL=-999):\n",
    "    \"\"\"Converts the cylindrical polar coordinates to cartesian coordinates for jets\"\"\"\n",
    "\n",
    "    # Seperate the pts, etas and phis\n",
    "    pts = pt_eta_phi[:, :, 0:1]\n",
    "    etas = pt_eta_phi[:, :, 1:2]\n",
    "    phis = pt_eta_phi[:, :, 2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    # Transforms only the non masked values from cylindrical to cartesian coordinates. Mask values are left unchanged.\n",
    "    mask1 = pts == MASKVAL \n",
    "    mask2 = phis == MASKVAL\n",
    "    mask3 = etas == MASKVAL\n",
    "    pxs = np.where(mask1 | mask2, pts, pts * np.cos(phis)) \n",
    "    pys = np.where(mask1 | mask2, pts, pts * np.sin(phis))\n",
    "    pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector in cartesian coordinates\n",
    "    return np.concatenate([pxs, pys, pzs], axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_eta_phi2_px_py_pz_predicted_tracks(predictions):\n",
    "    #Obtain the pts,etas and phis\n",
    "    pts = predictions[:,0:1]\n",
    "    etas = predictions[:,1:2]\n",
    "    phis = predictions[:,2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    # Transforms only the non masked values from cylindrical to cartesian coordinates. Mask values are left unchanged.\n",
    "    pxs =  pts * np.cos(phis)\n",
    "    pys =  pts * np.sin(phis)\n",
    "    pzs =  pts * np.sinh(etas)\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector in cartesian coordinates\n",
    "    return np.concatenate([pxs, pys, pzs], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from cylindrical to cartesian coordinates\n",
    "def pt_eta_phi_2_px_py_pz_jets(pt_eta_phi):\n",
    "    \"\"\"Converts the cylindrical polar coordinates to cartesian coordinates for jets\"\"\"\n",
    "\n",
    "    # Seperate the pts, etas and phis\n",
    "    pts = pt_eta_phi[:, 0:1]\n",
    "    etas = pt_eta_phi[:, 1:2]\n",
    "    phis = pt_eta_phi[:, 2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    pxs = pts * np.cos(phis)\n",
    "    pys = pts * np.sin(phis)\n",
    "    pzs = pts * np.sinh(etas)\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector\n",
    "    return np.concatenate([pxs, pys, pzs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x_values, maxsize, MASKVAL=-999):\n",
    "    \"\"\"\n",
    "    Pads the inputs with nans to get to the maxsize\n",
    "    \"\"\"\n",
    "    #Pad the non-regular arrays with null values until they are all of the same size. Then replace the nulls with MASVAL\n",
    "    y_values = ak.fill_none(ak.pad_none(x_values, maxsize, axis=1, clip=True), MASKVAL)[:, :maxsize]\n",
    "    return ak.to_regular(y_values, axis=1) #Return the regular arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x_values, maxsize=-1, MASKVAL=-999):\n",
    "    \"\"\"\"Pads the input to ensure they are all of regular size and then zips together result\"\"\"\n",
    "    y_values = {} \n",
    "    for field in x_values.fields:\n",
    "        z_values = x_values[field]\n",
    "        if maxsize > 0:\n",
    "            z_values = pad(z_values, maxsize, MASKVAL)\n",
    "        y_values[field] = z_values\n",
    "\n",
    "    return ak.zip(y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogNormal_Loss_Function(true,mean_convariance_matrix):\n",
    "    \n",
    "    \"\"\"A custom loss function designed to force the neural network \n",
    "    to return a prediction and associated uncertainty for target features\"\"\"\n",
    "\n",
    "    #Identify the number of target features\n",
    "    n_targets = np.shape(true)[1]\n",
    "\n",
    "    #Allocate the first n outputs of the dense layer to represent the mean\n",
    "    means = mean_convariance_matrix[:, :n_targets]\n",
    "\n",
    "    #Allocate the second n outputs of the dense layer to represent the variances\n",
    "    logvariances = mean_convariance_matrix[:, n_targets: 2* n_targets]\n",
    "\n",
    "    #Allocate the last n outputs of th4e dense layer to represent the covariances\n",
    "    logcovariances = mean_convariance_matrix[:, 2*n_targets:]\n",
    "\n",
    "\n",
    "    #Calculate the logNormal loss\n",
    "    sum_loss = 0\n",
    "    for target in range(n_targets):\n",
    "        sum_loss += (1/2)*keras.backend.log(2*np.pi) + logvariances[:,target] + ((true[:,target] - means[:,target])**2)/(2*keras.backend.exp(logvariances[:,target])**2)\n",
    "    \n",
    "    return sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is being stored in a tree datastructure.\n",
    "# We access the charm root using this command\n",
    "tree = uproot.open(\"hffrag.root:CharmAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "MASKVAL = -999 # This value is introduced to ensure arrays are regular (Of the same size). They will be masked later by the network\n",
    "MAXTRACKS = 32 # This value is the maximum number of tracks allowed per event\n",
    "BATCHSIZE = 64 # This is the batch size of the mini batches used during training\n",
    "EPOCHS = 100 # This is the default number of epochs for which the neural network will train providing that early stopping does not occur\n",
    "MAXEVENTS = 1e20 #This is the maximum number of events that will the program will accept\n",
    "LR = 1e-4 #This is the default learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we wish to study\n",
    "track_features = [\"AnalysisTracks_pt\", \"AnalysisTracks_eta\", \"AnalysisTracks_phi\", \"AnalysisTracks_z0sinTheta\",\n",
    "                  \"AnalysisTracks_d0sig\", \"AnalysisTracks_d0\", \"AnalysisTracks_d0sigPV\", \"AnalysisTracks_d0PV\"]\n",
    "jet_features = [\"AnalysisAntiKt4TruthJets_pt\", \"AnalysisAntiKt4TruthJets_eta\", \"AnalysisAntiKt4TruthJets_phi\",\n",
    "                \"AnalysisAntiKt4TruthJets_ghostB_pt\", \"AnalysisAntiKt4TruthJets_ghostB_eta\",\"AnalysisAntiKt4TruthJets_ghostB_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dat from the root file\n",
    "features = tree.arrays(jet_features+track_features, entry_stop=MAXEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the events of interest\n",
    "events = features[ak.sum(\n",
    "    features[\"AnalysisAntiKt4TruthJets_pt\"] > 25000, axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jets to train on is:  141329\n",
      "The number of track features is:  8\n"
     ]
    }
   ],
   "source": [
    "# Displays the number of jets being trained on\n",
    "jets = events[jet_features][:, 0]\n",
    "print(\"The number of jets to train on is: \", len(jets))\n",
    "print(\"The number of track features is: \",len(track_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tracks from the events\n",
    "tracks = events[track_features]\n",
    "\n",
    "# Match the tracks to the jets\n",
    "matchedtracks = tracks[Match_Tracks(jets, tracks)]\n",
    "\n",
    "# Pad and Flatten the data\n",
    "matchedtracks = flatten(matchedtracks, MAXTRACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 outputs\n",
      "There are 32 inputs\n"
     ]
    }
   ],
   "source": [
    "# Identify the the bottom jets and their associated tracks\n",
    "bjets = ak.sum(jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] > 5000, axis=1) > 0\n",
    "jets = jets[bjets]\n",
    "\n",
    "# Obtain the pt, eta and phi of each b hadron jet\n",
    "bhads_pt = jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"][:, 0].to_numpy()\n",
    "bhads_eta = jets[\"AnalysisAntiKt4TruthJets_ghostB_eta\"][:,0].to_numpy()\n",
    "bhads_phi = jets[\"AnalysisAntiKt4TruthJets_ghostB_phi\"][:,0].to_numpy()\n",
    "\n",
    "bhads = np.stack([bhads_pt,bhads_eta,bhads_phi],axis = -1) #Combine the momentum, eta and phi for each jet into one array\n",
    "\n",
    "print(\"There are {} outputs\".format(np.shape(bhads)[1])) # Display the number of target features the neural network will predict\n",
    "matchedtracks = matchedtracks[bjets]\n",
    "print(\"There are {} inputs\".format(np.shape(matchedtracks)[1])) # Display the number of target features the neural network will use in it's ppredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 3)\n",
      "(5,)\n",
      "[1.48e+05, 1.04e+05, 1.16e+05, 4.03e+04, ... 8.14e+04, 9.83e+04, 1.45e+05, 9.11e+04]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(bhads)) #Check the shape of the neural network\n",
    "print(np.shape(jet_features[:-1])) #Check for shape of the jet features\n",
    "print(jets[jet_features[0]]) # Check the jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 3)\n"
     ]
    }
   ],
   "source": [
    "# Transform the jet and tracks to unstructed data.\n",
    "jets = structured_to_unstructured(jets[jet_features[:-3]])\n",
    "matchedtracks = structured_to_unstructured(matchedtracks)\n",
    "print(np.shape(jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.47e+04, 0.753, 1.14, 1.19, 75.5, ... -0.165, -0.51, -0.0283, -0.692, -0.038]]]\n",
      "(68143, 32)\n"
     ]
    }
   ],
   "source": [
    "#Check the matchtracks are the correct shape\n",
    "print(matchedtracks[:, 0:1])\n",
    "print(np.shape(matchedtracks[:, :, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 32, 3)\n",
      "(68143, 32, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21007/1222727277.py:16: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n"
     ]
    }
   ],
   "source": [
    "# Convert the coordinates of the b jets and tracks to cartesian coordinates\n",
    "tracks_p = pt_eta_phi_2_px_py_pz_tracks(matchedtracks.to_numpy())\n",
    "bhads = pt_eta_phi_2_px_py_pz_jets(bhads)\n",
    "\n",
    "#Check the shape of the momenta of the tracks and the rest of the data is consistent\n",
    "print(np.shape(tracks_p))\n",
    "print(np.shape(matchedtracks[:, :, 3:]))\n",
    "\n",
    "#Combine the momenta of the tracks with the rest of the track features to form the track dataset\n",
    "tracks = np.concatenate([tracks_p,matchedtracks[:,:,3:].to_numpy()],axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 32, 8)\n",
      "(68143, 3)\n",
      "[6.20926450e+03 1.33553447e+04 1.21693980e+04 1.18753994e+00\n",
      " 7.55359192e+01 1.33110714e+00 8.57456207e+01 1.32391548e+00]\n",
      "[ 48855.56531144 128363.19160447 124938.01790683]\n"
     ]
    }
   ],
   "source": [
    "#Check that this is all the correct shape\n",
    "print(np.shape(tracks))\n",
    "print(np.shape(bhads))\n",
    "print(tracks[0,0])\n",
    "print(bhads[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 32, 8)\n",
      "[0.26189855 0.38175348 0.10230412 1.42959932 1.58783932 1.42992229\n",
      " 1.60955267 1.42990682]\n"
     ]
    }
   ],
   "source": [
    "Scaler = StandardScaler()\n",
    "Num_events,Num_tracks,Num_features = np.shape(tracks)\n",
    "tracks = np.reshape(tracks, newshape=(-1,Num_features))\n",
    "tracks = Scaler.fit_transform(tracks)\n",
    "tracks = np.reshape(tracks, newshape= (Num_events,Num_tracks,Num_features))\n",
    "print(np.shape(tracks))\n",
    "print(tracks[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    tracks, bhads, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleEmbbedder(tf.keras.layers.Layer):\n",
    "    def __init__(self, track_layers,BATCH_SIZE, MASKVAL):\n",
    "        super().__init__()\n",
    "        self.track_layers = track_layers\n",
    "        self.MASKVAL = MASKVAL\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.mask = tf.keras.layers.Masking(mask_value=MASKVAL)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(track_layers[0],activation = \"elu\")),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(track_layers[1],activation = \"elu\")),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(track_layers[2],activation = \"elu\")),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(track_layers[3],activation = \"elu\")),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.BATCH_SIZE, activation = \"elu\"))\n",
    "        ])\n",
    "        \n",
    "    def call(self,x):\n",
    "        outputs = self.mask(x)\n",
    "        outputs = self.ffn(x)\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BhadEmbedder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, MASKVAL):\n",
    "        super().__init__()\n",
    "        self.MASKVAL = MASKVAL\n",
    "        self.d_model = d_model\n",
    "        self.mask = tf.keras.layers.Masking(mask_value=MASKVAL)\n",
    "        self.embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_model/8),\n",
    "            tf.keras.layers.Dense(d_model/4),\n",
    "            tf.keras.layers.Dense(d_model/2),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "    def call(self,x):\n",
    "        outputs = self.mask(x)\n",
    "        outputs = self.embedding(outputs)\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jet_Layer(keras.layers.Layer):\n",
    "    def __init__(self, dff, n_targets,d_model, MASKVAL=-999):\n",
    "        super().__init__()\n",
    "        self.mask = tf.keras.layers.Masking()\n",
    "        self.n_targets = n_targets\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "        layers.Dense(self.dff, activation = \"elu\"),\n",
    "        layers.Dense(self.d_model),\n",
    "        layers.Dense(self.n_targets+self.n_targets*(self.n_targets+1)//2),\n",
    "        ])\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.mask(x)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8)\n",
      "54514\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,MASKVAL, **kwargs):\n",
    "        super().__init__()\n",
    "        self.masking = tf.keras.layers.Masking(MASKVAL)\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self,x):\n",
    "        attn_output = self.mha(\n",
    "            query = x,\n",
    "            value = x,\n",
    "            key = x,\n",
    "        )\n",
    "        x = self.masking(x)\n",
    "        x  = self.add([x,attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    def call(self,x,context):\n",
    "        attn_output, attn_scores = self.mha(query = x, key = context, \n",
    "        value = context,\n",
    "        return_attention_scores = True)\n",
    "    \n",
    "        self.last_attn_scores = attn_scores\n",
    "        \n",
    "        x = self.masking(x)\n",
    "        x = self.add([x,attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self,x):\n",
    "        attn_output = self.mha(\n",
    "            query = x,\n",
    "            value = x,\n",
    "            key = x,\n",
    "            use_causal_mask = True)\n",
    "            \n",
    "        x = self.masking(x)\n",
    "        x = self.add([x,attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model, dff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff,activation = 'gelu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.add([x,self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, MASKVAL = -999, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            MASKVAL = MASKVAL,\n",
    "            num_heads = num_heads,\n",
    "            key_dim = d_model,\n",
    "            dropout = dropout_rate\n",
    "        )\n",
    "    \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BhadAttentionBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1,\n",
    "               MASKVAL = MASKVAL):\n",
    "    super(BhadAttentionBlock, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate,\n",
    "        MASKVAL= MASKVAL)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate,\n",
    "        MASKVAL=MASKVAL)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, track_layers,num_layers, d_model, num_heads, dff,MASKVAL = -999, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.ParticleEmbbedder = ParticleEmbbedder(track_layers,d_model,MASKVAL)\n",
    "        \n",
    "        self.encoder_layers = [\n",
    "            ParticleAttentionBlock(\n",
    "            d_model=d_model,\n",
    "            MASKVAL = MASKVAL,\n",
    "            num_heads = num_heads,\n",
    "            dff = dff,\n",
    "            dropout_rate = dropout_rate\n",
    "            )\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.ParticleEmbbedder(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for increment in range(self.num_layers):\n",
    "            x = self.encoder_layers[increment](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, n_targets, MASKVAL = MASKVAL,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    self.BhadEmbedder = BhadEmbedder( d_model = d_model, MASKVAL=MASKVAL)\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        BhadAttentionBlock(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.BhadEmbedder(x)\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for increment in range(self.num_layers):\n",
    "      x  = self.dec_layers[increment](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    return Sum()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hffragTransformer(tf.keras.Model):\n",
    "    def __init__(self, *,track_layers, num_layers, d_model, num_heads, dff, n_targets, MASKVAL = MASKVAL, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(track_layers = track_layers, d_model = d_model, num_layers= num_layers, num_heads=num_heads,dff = dff, MASKVAL= MASKVAL, dropout_rate = dropout)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff = dff, n_targets=n_targets, MASKVAL=MASKVAL, dropout_rate=dropout)\n",
    "\n",
    "        self.jet_layer = Jet_Layer(n_targets= n_targets, dff = dff, d_model= d_model)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "\n",
    "        context = self.encoder(context)\n",
    "\n",
    "        x = self.decoder(x, context)\n",
    "\n",
    "        outputs = self.jet_layer(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 00:37:48.204131: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-16 00:37:48.204163: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-16 00:37:48.204183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vonneumann.csc.warwick.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2023-01-16 00:37:48.204878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 128)\n"
     ]
    }
   ],
   "source": [
    "track_layers = [32,32,32,32,32]\n",
    "BATCHSIZE = 128\n",
    "sample_particle_embedder = ParticleEmbbedder(track_layers,BATCHSIZE,MASKVAL)\n",
    "output = sample_particle_embedder(X_train)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 3, 1)\n",
      "(54514, 64)\n"
     ]
    }
   ],
   "source": [
    "sample_Bhad = BhadEmbedder( d_model = 64, MASKVAL=MASKVAL)\n",
    "sample = np.reshape(y_train, (y_train.shape[0], y_train.shape[1], 1))\n",
    "print(sample.shape)\n",
    "sample_2 = sample_Bhad(y_train)\n",
    "print(sample_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8)\n",
      "(54514, 32, 8)\n"
     ]
    }
   ],
   "source": [
    "sample_gsa = GlobalSelfAttention(MASKVAL = -999, num_heads = 8, key_dim = 3)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(sample_gsa(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8)\n",
      "(54514, 32, 8)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder_layer = ParticleAttentionBlock(d_model = 8, num_heads=8, dff = 2048)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(sample_encoder_layer(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8)\n",
      "(54514, 32, 128)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(track_layers = track_layers, num_layers=4, d_model = 128, num_heads= 8, dff = 2048, MASKVAL=MASKVAL)\n",
    "sample_encoder_output = sample_encoder(X_train)\n",
    "print(X_train.shape)\n",
    "print(sample_encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 3, 1)\n",
      "(13629, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train_1 = np.reshape(y_train, (y_train.shape[0], y_train.shape[1],1 ))\n",
    "print(y_train_1.shape)\n",
    "y_valid_1 = np.reshape(y_valid, (y_valid.shape[0], y_valid.shape[1],1 ))\n",
    "print(y_valid_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8)\n",
      "(54514, 3)\n",
      "(54514, 128)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers= 4, d_model = 128, n_targets= 3, num_heads= 8, MASKVAL = MASKVAL, dff = 128)\n",
    "\n",
    "output = sample_decoder(x = y_train_1, context = X_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 9)\n"
     ]
    }
   ],
   "source": [
    "sample_jet_layer = Jet_Layer(n_targets=3,dff = 512, d_model=128,MASKVAL=-999)\n",
    "output = sample_jet_layer(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_layers = [32,32,32,32]\n",
    "num_layers = 4\n",
    "num_heads = 9\n",
    "dff = 128\n",
    "MASKVAL = -999\n",
    "dropout = 0.1\n",
    "n_targets = bhads.shape[1]\n",
    "d_model = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_hffragTransformer = hffragTransformer(\n",
    "    track_layers=track_layers,\n",
    "    num_layers = num_layers,\n",
    "    dff = dff,\n",
    "    num_heads=num_heads,\n",
    "    MASKVAL = MASKVAL,\n",
    "    dropout= dropout,\n",
    "    n_targets= n_targets,\n",
    "    d_model = d_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sample_hffragTransformer((X_train,y_train_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8)\n",
      "(54514, 9)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hffrag_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  23208     \n",
      "                                                                 \n",
      " decoder_2 (Decoder)         multiple                  29210     \n",
      "                                                                 \n",
      " jet__layer_1 (Jet_Layer)    multiple                  2265      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,683\n",
      "Trainable params: 54,427\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample_hffragTransformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_hffragTransformer.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(LR),\n",
    "    loss = Mean_Squared_Error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "426/426 [==============================] - 64s 76ms/step - loss: 47476047872.0000 - val_loss: 48103899136.0000\n",
      "Epoch 2/100\n",
      "426/426 [==============================] - 30s 69ms/step - loss: 47456686080.0000 - val_loss: 48099946496.0000\n",
      "Epoch 3/100\n",
      "156/426 [=========>....................] - ETA: 17s - loss: 47052754944.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21007/3585465661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = sample_hffragTransformer.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCHSIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = sample_hffragTransformer.fit(\n",
    "    (X_train,y_train_1), y_train,\n",
    "    validation_data= ((X_valid,y_valid_1),y_valid),\n",
    "    batch_size=BATCHSIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a05ddcd8ffea9a6a7d2e914b733df5445b717626b5b8c92c04bfc4eb6e7f5cba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
