{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:30:27.966758: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-13 12:30:28.129770: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-13 12:30:28.134093: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-13 12:30:28.134109: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-13 12:30:29.257543: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-13 12:30:29.257615: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-13 12:30:29.257622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of jets to train on:\n",
      "141329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/physics/phujdj/DeepLearningParticlePhysics/hffrag.py:136: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = numpy.where(mask, pts, pts * numpy.sinh(etas))\n",
      "/home/physics/phujdj/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from keras import callbacks\n",
    "import keras\n",
    "import DeepSetNeuralNetArchitecture as DSNNA\n",
    "import ParticleTransformer as ParT\n",
    "from DeepSetNeuralNetArchitecture import PredictOnEpoch\n",
    "import ConvolutionalRecurrentNeuralNetworkArchitecture as CRNNA\n",
    "from DeepSetNeuralNetArchitecture import LogNormal_Loss_Function\n",
    "from DeepSetNeuralNetArchitecture import Mean_Squared_Error\n",
    "import keras.backend as k\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import sklearn as sk\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from hffrag import fixedbinning\n",
    "from hffrag import binneddensity\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaylendarnbrook\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/physics/phujdj/DeepLearningParticlePhysics/wandb/run-20230113_123101-2bkw33r8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kaylendarnbrook/hffrag-DeepNetNeuralNetworkArchitecture/runs/2bkw33r8\" target=\"_blank\">efficient-thunder-5</a></strong> to <a href=\"https://wandb.ai/kaylendarnbrook/hffrag-DeepNetNeuralNetworkArchitecture\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kaylendarnbrook/hffrag-DeepNetNeuralNetworkArchitecture/runs/2bkw33r8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f63ad8b5b80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project = \"hffrag-DeepNetNeuralNetworkArchitecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is being stored in a tree datastructure.\n",
    "# We access the charm root using this command\n",
    "tree = uproot.open(\"hffrag.root:CharmAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "MASKVAL = -999 # This value is introduced to ensure arrays are regular (Of the same size). They will be masked later by the network\n",
    "MAXTRACKS = 32 # This value is the maximum number of tracks allowed per event\n",
    "BATCHSIZE = 64 # This is the batch size of the mini batches used during training\n",
    "EPOCHS = 100 # This is the default number of epochs for which the neural network will train providing that early stopping does not occur\n",
    "MAXEVENTS = 1e20 #This is the maximum number of events that will the program will accept\n",
    "LR = 1e-4 #This is the default learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\":EPOCHS,\n",
    "    \"batch_size\":BATCHSIZE,\n",
    "    \"max_events\": MAXEVENTS,\n",
    "    \"MAXTRACKS\": MAXTRACKS, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we wish to study\n",
    "track_features = [\"AnalysisTracks_pt\", \"AnalysisTracks_eta\", \"AnalysisTracks_phi\", \"AnalysisTracks_z0sinTheta\",\n",
    "                  \"AnalysisTracks_d0sig\", \"AnalysisTracks_d0\", \"AnalysisTracks_d0sigPV\", \"AnalysisTracks_d0PV\"]\n",
    "jet_features = [\"AnalysisAntiKt4TruthJets_pt\", \"AnalysisAntiKt4TruthJets_eta\", \"AnalysisAntiKt4TruthJets_phi\",\n",
    "                \"AnalysisAntiKt4TruthJets_ghostB_pt\", \"AnalysisAntiKt4TruthJets_ghostB_eta\",\"AnalysisAntiKt4TruthJets_ghostB_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dat from the root file\n",
    "features = tree.arrays(jet_features+track_features, entry_stop=MAXEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the events of interest\n",
    "events = features[ak.sum(\n",
    "    features[\"AnalysisAntiKt4TruthJets_pt\"] > 25000, axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jets to train on is:  141329\n",
      "The number of track features is:  8\n"
     ]
    }
   ],
   "source": [
    "# Displays the number of jets being trained on\n",
    "jets = events[jet_features][:, 0]\n",
    "print(\"The number of jets to train on is: \", len(jets))\n",
    "print(\"The number of track features is: \",len(track_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tracks from the events\n",
    "tracks = events[track_features]\n",
    "\n",
    "# Match the tracks to the jets\n",
    "matchedtracks = tracks[DSNNA.Match_Tracks(jets, tracks)]\n",
    "\n",
    "# Pad and Flatten the data\n",
    "matchedtracks = DSNNA.flatten(matchedtracks, MAXTRACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 outputs\n",
      "There are 32 inputs\n"
     ]
    }
   ],
   "source": [
    "# Identify the the bottom jets and their associated tracks\n",
    "bjets = ak.sum(jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] > 5000, axis=1) > 0\n",
    "jets = jets[bjets]\n",
    "\n",
    "# Obtain the pt, eta and phi of each b hadron jet\n",
    "bhads_pt = jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"][:, 0].to_numpy()\n",
    "bhads_eta = jets[\"AnalysisAntiKt4TruthJets_ghostB_eta\"][:,0].to_numpy()\n",
    "bhads_phi = jets[\"AnalysisAntiKt4TruthJets_ghostB_phi\"][:,0].to_numpy()\n",
    "\n",
    "bhads = np.stack([bhads_pt,bhads_eta,bhads_phi],axis = -1) #Combine the momentum, eta and phi for each jet into one array\n",
    "\n",
    "print(\"There are {} outputs\".format(np.shape(bhads)[1])) # Display the number of target features the neural network will predict\n",
    "matchedtracks = matchedtracks[bjets]\n",
    "print(\"There are {} inputs\".format(np.shape(matchedtracks)[1])) # Display the number of target features the neural network will use in it's ppredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the jet and tracks to unstructed data.\n",
    "jets = structured_to_unstructured(jets[jet_features[:-3]])\n",
    "matchedtracks = structured_to_unstructured(matchedtracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/physics/phujdj/DeepLearningParticlePhysics/DeepSetNeuralNetArchitecture.py:103: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n"
     ]
    }
   ],
   "source": [
    "# Convert the coordinates of the b jets and tracks to cartesian coordinates\n",
    "tracks_p = DSNNA.pt_eta_phi_2_px_py_pz_tracks(matchedtracks.to_numpy())\n",
    "bhads = DSNNA.pt_eta_phi_2_px_py_pz_jets(bhads)\n",
    "\n",
    "#Combine the momenta of the tracks with the rest of the track features to form the track dataset\n",
    "tracks = np.concatenate([tracks_p,matchedtracks[:,:,3:].to_numpy()],axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaler = StandardScaler()\n",
    "Num_events,Num_tracks,Num_features = np.shape(tracks)\n",
    "tracks = np.reshape(tracks, newshape=(-1,Num_features))\n",
    "tracks = Scaler.fit_transform(tracks)\n",
    "tracks = np.reshape(tracks, newshape= (Num_events,Num_tracks,Num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    tracks, bhads, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8) (54514, 3)\n",
      "(1, 32, 8) (1, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_event, y_train_event = np.array([X_train[0]]), np.array([y_train[0]])\n",
    "X_valid_event, y_valid_event = np.array([X_valid[0]]), np.array([y_valid[0]])\n",
    "print(np.shape(X_train),np.shape(y_train))\n",
    "print(np.shape(X_train_event),np.shape(y_train_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8) (13629, 32, 8)\n",
      "(54514, 3) (13629, 3)\n"
     ]
    }
   ],
   "source": [
    "#Check for the of the training and validation sets\n",
    "print(np.shape(X_train), np.shape(X_valid))\n",
    "print(np.shape(y_train), np.shape(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the training and validation datasets.\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/X_train_data.npy\",X_train)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/X_valid_data.npy\",X_valid)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/y_train_data.npy\",y_train)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/y_valid_data.npy\",y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 64, 64, 64, 64, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:32:26.845858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-13 12:32:26.845889: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-13 12:32:26.845910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vonneumann.csc.warwick.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2023-01-13 12:32:26.847372: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Cyclical Learning Rate Scheduler:\n",
    "steps_per_epoch = len(X_train)\n",
    "clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate = 1e-4,\n",
    "maximal_learning_rate = 0.01,\n",
    "scale_fn = lambda x: 1/(2**(x-1)),\n",
    "step_size = 2.0 * steps_per_epoch\n",
    ")\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs = {}):\n",
    "        self.logs = []\n",
    "    def on_epoch_begin(self, epoch, logs ={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        self.logs.append(timer() - self.starttime)\n",
    "        \n",
    "# Builds the deep neural network\n",
    "track_layers = [64,64,64,64,64]\n",
    "jet_layers = [128,128,128,128,128]\n",
    "\n",
    "len1 = [len(track_features)]+track_layers\n",
    "print(len1)\n",
    "\n",
    "#Initializers the optimizer used for training the network\n",
    "optimizer = tf.keras.optimizers.Nadam(LR)\n",
    "optimizer_Constant = tf.keras.optimizers.SGD(learning_rate = 1e-4, momentum = 0.9, clipnorm = 1.0, nesterov = True )\n",
    "\n",
    "#Builds the DeepSet Neural Network\n",
    "DeepNet = DSNNA.DeepSetNeuralNetwork(\n",
    "    [len(track_features)] + track_layers, jet_layers,np.shape(y_train)[1],optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 8)]         0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 8)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 8)          72        \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, None, 8)          32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 64)         576       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, None, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, None, 64)         4160      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, None, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, None, 64)         4160      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, None, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, None, 64)         4160      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, None, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, None, 64)         4160      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " sum (Sum)                   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 9)                 1161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,433\n",
      "Trainable params: 94,625\n",
      "Non-trainable params: 1,808\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Summarises the Deep Set Neural Network Architecture\n",
    "DeepNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(DeepNet, to_file =\"NetworkArchitecture.png\", show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    }
   ],
   "source": [
    "# Introduce early_stopping to prevent overfitting\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.00001,  # The minimum amount of change to count as an improvement\n",
    "    patience=5,  # The number of epochs to wait before stopping\n",
    "    restore_best_weights=True,  # Keep the best weights\n",
    ")\n",
    "# Prevent spikes in the validation and training loss due to the gradient descent kicking the network out of a local minima\n",
    "reduce_learn_on_plateau = callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.80, patience=10, min_lr=1e-8)\n",
    "\n",
    "# Save the weights of the model to allow reuse in future.\n",
    "path = \"/home/physics/phujdj/DeepLearningParticlePhysics/CheckPointsDeepNet/DeepNetWeights&Biases.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                 save_weights_only=True, verbose=0, save_freq = 100*BATCHSIZE)\n",
    "#Timer\n",
    "cb = TimingCallback()\n",
    "\n",
    "#Weight&Biases Callback:\n",
    "Wanda = WandbCallback(save_graph = True,save_weights_only = True, log_weights = True, log_gradients = True, log_evaluation = True, training_data = (X_train,y_train), validation_data = (X_valid,y_valid), log_batch_frequency = 5)\n",
    "\n",
    "# Learning Scheduler:\n",
    "exponential_decay_fn = DSNNA.expontial_decay(lr0 = LR,s = 30)\n",
    "learning_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epilson = tf.constant(keras.backend.epsilon(), shape = (np.shape(bhads)[0],))\n",
    "constant = tf.ones(shape = (np.shape(bhads)[0],) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(68143,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.log(tf.experimental.numpy.maximum(constant,epilson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:32:33.373081: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f62080c1120 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-13 12:32:33.373119: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2023-01-13 12:32:33.599116: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15/852 [..............................] - ETA: 6s - loss: 41803259904.0000    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:32:38.326281: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1704/1704 [==============================] - 4s 2ms/step\n",
      "852/852 [==============================] - 28s 21ms/step - loss: 47474421760.0000 - val_loss: 48101036032.0000 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47468744704.0000 - val_loss: 48103833600.0000 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47462547456.0000 - val_loss: 48096227328.0000 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47455653888.0000 - val_loss: 48098951168.0000 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47448043520.0000 - val_loss: 48089448448.0000 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47439613952.0000 - val_loss: 48066736128.0000 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47430586368.0000 - val_loss: 48063012864.0000 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47420862464.0000 - val_loss: 48082419712.0000 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47410405376.0000 - val_loss: 48079491072.0000 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47399239680.0000 - val_loss: 48040407040.0000 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47387377664.0000 - val_loss: 48068988928.0000 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47374905344.0000 - val_loss: 48062824448.0000 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47361859584.0000 - val_loss: 48028708864.0000 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47348150272.0000 - val_loss: 48000212992.0000 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47334117376.0000 - val_loss: 47964553216.0000 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47318925312.0000 - val_loss: 47975604224.0000 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47303122944.0000 - val_loss: 48020348928.0000 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47286874112.0000 - val_loss: 47999401984.0000 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47270027264.0000 - val_loss: 47986094080.0000 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47252602880.0000 - val_loss: 47973261312.0000 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47233966080.0000 - val_loss: 47970004992.0000 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47214682112.0000 - val_loss: 47971930112.0000 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47195303936.0000 - val_loss: 47882899456.0000 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47175323648.0000 - val_loss: 47829270528.0000 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "1704/1704 [==============================] - 5s 3ms/step\n",
      "852/852 [==============================] - 16s 19ms/step - loss: 47154085888.0000 - val_loss: 47891611648.0000 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47132471296.0000 - val_loss: 47957655552.0000 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47110840320.0000 - val_loss: 47927914496.0000 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47087730688.0000 - val_loss: 47857438720.0000 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 12s 14ms/step - loss: 47064612864.0000 - val_loss: 47788654592.0000 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47040040960.0000 - val_loss: 47819186176.0000 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 47016210432.0000 - val_loss: 47777374208.0000 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46989914112.0000 - val_loss: 47865892864.0000 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46964350976.0000 - val_loss: 47917375488.0000 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46937694208.0000 - val_loss: 47722315776.0000 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46911631360.0000 - val_loss: 47754125312.0000 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46882361344.0000 - val_loss: 47739170816.0000 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46854758400.0000 - val_loss: 47818338304.0000 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46824599552.0000 - val_loss: 47784722432.0000 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46794891264.0000 - val_loss: 47768023040.0000 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46766788608.0000 - val_loss: 47732715520.0000 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46733496320.0000 - val_loss: 47667945472.0000 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46701604864.0000 - val_loss: 47659597824.0000 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "1704/1704 [==============================] - 4s 2ms/step\n",
      "852/852 [==============================] - 13s 15ms/step - loss: 46671945728.0000 - val_loss: 47516737536.0000 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "1704/1704 [==============================] - 4s 2ms/step\n",
      "852/852 [==============================] - 14s 16ms/step - loss: 46636457984.0000 - val_loss: 47344758784.0000 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "1704/1704 [==============================] - 4s 2ms/step\n",
      "852/852 [==============================] - 14s 17ms/step - loss: 46605213696.0000 - val_loss: 47791259648.0000 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "1704/1704 [==============================] - 4s 2ms/step\n",
      "852/852 [==============================] - 14s 17ms/step - loss: 46571143168.0000 - val_loss: 47619129344.0000 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "1704/1704 [==============================] - 4s 2ms/step\n",
      "852/852 [==============================] - 13s 16ms/step - loss: 46537035776.0000 - val_loss: 47538302976.0000 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 12s 14ms/step - loss: 46499999744.0000 - val_loss: 47602950144.0000 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46465220608.0000 - val_loss: 47246102528.0000 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46427172864.0000 - val_loss: 47361363968.0000 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46391504896.0000 - val_loss: 47738523648.0000 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46353088512.0000 - val_loss: 47530889216.0000 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46315372544.0000 - val_loss: 47550812160.0000 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46275141632.0000 - val_loss: 47006486528.0000 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46238490624.0000 - val_loss: 47257739264.0000 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46198661120.0000 - val_loss: 47165427712.0000 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46157549568.0000 - val_loss: 47439081472.0000 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46115246080.0000 - val_loss: 47280005120.0000 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46073364480.0000 - val_loss: 47512645632.0000 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 46031814656.0000 - val_loss: 47396466688.0000 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45988511744.0000 - val_loss: 47356596224.0000 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45944119296.0000 - val_loss: 47332622336.0000 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45899788288.0000 - val_loss: 47351398400.0000 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45854896128.0000 - val_loss: 47346855936.0000 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45808844800.0000 - val_loss: 47136018432.0000 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45764694016.0000 - val_loss: 47412084736.0000 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45719699456.0000 - val_loss: 47261356032.0000 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45670588416.0000 - val_loss: 47175675904.0000 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45623083008.0000 - val_loss: 47322013696.0000 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45579653120.0000 - val_loss: 47301705728.0000 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45527875584.0000 - val_loss: 47158292480.0000 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45481582592.0000 - val_loss: 47399755776.0000 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45427040256.0000 - val_loss: 46492811264.0000 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 45376638976.0000 - val_loss: 47289040896.0000 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 10s 12ms/step - loss: 45325656064.0000 - val_loss: 47285768192.0000 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 10s 12ms/step - loss: 45276131328.0000 - val_loss: 46660874240.0000 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 10s 12ms/step - loss: 45220052992.0000 - val_loss: 46602510336.0000 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 12ms/step - loss: 45168754688.0000 - val_loss: 46462550016.0000 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 12ms/step - loss: 45115396096.0000 - val_loss: 47089393664.0000 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 10s 12ms/step - loss: 45060317184.0000 - val_loss: 46643257344.0000 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 10s 12ms/step - loss: 45006176256.0000 - val_loss: 46711242752.0000 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 10s 12ms/step - loss: 44957732864.0000 - val_loss: 46947627008.0000 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 10s 12ms/step - loss: 44897173504.0000 - val_loss: 46842859520.0000 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 10s 12ms/step - loss: 44842848256.0000 - val_loss: 46929870848.0000 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 44781367296.0000 - val_loss: 46517137408.0000 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 12ms/step - loss: 44728156160.0000 - val_loss: 46110285824.0000 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 44666331136.0000 - val_loss: 46227111936.0000 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 12ms/step - loss: 44610301952.0000 - val_loss: 46703951872.0000 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 44555112448.0000 - val_loss: 45934632960.0000 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 44494348288.0000 - val_loss: 45859106816.0000 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "1704/1704 [==============================] - 3s 2ms/step\n",
      "852/852 [==============================] - 11s 13ms/step - loss: 44434329600.0000 - val_loss: 45596172288.0000 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      " 29/852 [>.............................] - ETA: 6s - loss: 45392207872.0000"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "history = DeepNet.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid,y_valid),\n",
    "    batch_size=BATCHSIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping,reduce_learn_on_plateau,PredictOnEpoch(DeepNet,X_train,y_train),cb,cp_callback],\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and validation curves vs epoch\n",
    "history_df = pd.DataFrame(history.history)\n",
    "np.log(history_df.loc[:, [\"loss\",\"val_loss\"]]).plot()\n",
    "history_df.to_csv('/home/physics/phujdj/DeepLearningParticlePhysics/history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(cb.logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to the console the minimum epoch\n",
    "print(\"Minimum validation loss: {}\".format(history_df[\"loss\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the entire performance of the model\n",
    "loss = DeepNet.evaluate(tracks,bhads,verbose = 2)\n",
    "print(\"The Loaded DeepNet has loss: \", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('SpocFit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a05ddcd8ffea9a6a7d2e914b733df5445b717626b5b8c92c04bfc4eb6e7f5cba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
