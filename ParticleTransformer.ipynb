{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import keras.layers as layers\n",
    "from keras import regularizers\n",
    "import keras\n",
    "from Sum import Sum\n",
    "import matplotlib.pyplot as plt\n",
    "from hffrag import fixedbinning\n",
    "from hffrag import binneddensity\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "MASKVAL = -999 # This value is introduced to ensure arrays are regular (Of the same size). They will be masked later by the network\n",
    "MAXTRACKS = 32 # This value is the maximum number of tracks allowed per event\n",
    "BATCHSIZE = 64 # This is the batch size of the mini batches used during training\n",
    "EPOCHS = 1000 # This is the default number of epochs for which the neural network will train providing that early stopping does not occur\n",
    "MAXEVENTS = 1e20 #This is the maximum number of events that will the program will accept\n",
    "LR = 1e-4 #This is the default learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the associated tracks for each jet\n",
    "def Match_Tracks(jets, tracks):\n",
    "    \"\"\"Used to determine if a set of tracks belong to a particular set of jets\"\"\"\n",
    "\n",
    "    jet_eta = jets[\"AnalysisAntiKt4TruthJets_eta\"]\n",
    "    jet_phi = jets[\"AnalysisAntiKt4TruthJets_phi\"] \n",
    "\n",
    "    tracks_eta = tracks[\"AnalysisTracks_eta\"]\n",
    "    tracks_phi = tracks[\"AnalysisTracks_phi\"]\n",
    "\n",
    "    delta_etas = jet_eta - tracks_eta\n",
    "    delta_phis = np.abs(jet_phi - tracks_phi)\n",
    "\n",
    "    # Map the phis from a cyclical period onto a linear relation\n",
    "    ak.where(delta_phis > np.pi, delta_phis - np.pi, delta_phis)\n",
    "\n",
    "    # Returns a list of true and false, determining which tracks belong to those jets.\n",
    "    return np.sqrt(delta_phis**2 + delta_etas**2) < 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from cylindrical to cartesian coordinates\n",
    "def pt_eta_phi_2_px_py_pz_jets(pt_eta_phi):\n",
    "    \"\"\"Converts the cylindrical polar coordinates to cartesian coordinates for jets\"\"\"\n",
    "\n",
    "    # Seperate the pts, etas and phis\n",
    "    pts = pt_eta_phi[:, 0:1]\n",
    "    etas = pt_eta_phi[:, 1:2]\n",
    "    phis = pt_eta_phi[:, 2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    pxs = pts * np.cos(phis)\n",
    "    pys = pts * np.sin(phis)\n",
    "    pzs = pts * np.sinh(etas)\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector\n",
    "    return np.concatenate([pxs, pys, pzs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_eta_phi_2_px_py_pz_tracks(pt_eta_phi, MASKVAL=-999):\n",
    "    \"\"\"Converts the cylindrical polar coordinates to cartesian coordinates for jets\"\"\"\n",
    "\n",
    "    # Seperate the pts, etas and phis\n",
    "    pts = pt_eta_phi[:, :, 0:1]\n",
    "    etas = pt_eta_phi[:, :, 1:2]\n",
    "    phis = pt_eta_phi[:, :, 2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    # Transforms only the non masked values from cylindrical to cartesian coordinates. Mask values are left unchanged.\n",
    "    mask1 = pts == MASKVAL \n",
    "    mask2 = phis == MASKVAL\n",
    "    mask3 = etas == MASKVAL\n",
    "    pxs = np.where(mask1 | mask2, pts, pts * np.cos(phis)) \n",
    "    pys = np.where(mask1 | mask2, pts, pts * np.sin(phis))\n",
    "    pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector in cartesian coordinates\n",
    "    return np.concatenate([pxs, pys, pzs], axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_eta_phi2_px_py_pz_predicted_tracks(predictions):\n",
    "    #Obtain the pts,etas and phis\n",
    "    pts = predictions[:,0:1]\n",
    "    etas = predictions[:,1:2]\n",
    "    phis = predictions[:,2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    # Transforms only the non masked values from cylindrical to cartesian coordinates. Mask values are left unchanged.\n",
    "    pxs =  pts * np.cos(phis)\n",
    "    pys =  pts * np.sin(phis)\n",
    "    pzs =  pts * np.sinh(etas)\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector in cartesian coordinates\n",
    "    return np.concatenate([pxs, pys, pzs], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x_values, maxsize, MASKVAL=-999):\n",
    "    \"\"\"\n",
    "    Pads the inputs with nans to get to the maxsize\n",
    "    \"\"\"\n",
    "    #Pad the non-regular arrays with null values until they are all of the same size. Then replace the nulls with MASVAL\n",
    "    y_values = ak.fill_none(ak.pad_none(x_values, maxsize, axis=1, clip=True), MASKVAL)[:, :maxsize]\n",
    "    return ak.to_regular(y_values, axis=1) #Return the regular arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x_values, maxsize=-1, MASKVAL=-999):\n",
    "    \"\"\"\"Pads the input to ensure they are all of regular size and then zips together result\"\"\"\n",
    "    y_values = {} \n",
    "    for field in x_values.fields:\n",
    "        z_values = x_values[field]\n",
    "        if maxsize > 0:\n",
    "            z_values = pad(z_values, maxsize, MASKVAL)\n",
    "        y_values[field] = z_values\n",
    "\n",
    "    return ak.zip(y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogNormal_Loss_Function(true,mean_convariance_matrix):\n",
    "    \n",
    "    \"\"\"A custom loss function designed to force the neural network \n",
    "    to return a prediction and associated uncertainty for target features\"\"\"\n",
    "\n",
    "    #Identify the number of target features\n",
    "    n_targets = np.shape(true)[1]\n",
    "\n",
    "    #Allocate the first n outputs of the dense layer to represent the mean\n",
    "    means = mean_convariance_matrix[:, :n_targets]\n",
    "\n",
    "    #Allocate the second n outputs of the dense layer to represent the variances\n",
    "    logvariances = mean_convariance_matrix[:, n_targets: 2* n_targets]\n",
    "\n",
    "    #Allocate the last n outputs of th4e dense layer to represent the covariances\n",
    "    logcovariances = mean_convariance_matrix[:, 2*n_targets:]\n",
    "\n",
    "\n",
    "    #Calculate the logNormal loss\n",
    "    sum_loss = 0\n",
    "    for target in range(n_targets):\n",
    "        sum_loss += (1/2)*keras.backend.log(2*np.pi) + logvariances[:,target] + ((true[:,target] - means[:,target])**2)/(2*keras.backend.exp(logvariances[:,target])**2)\n",
    "    \n",
    "    return sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Root_Mean_Square_Metric(true, mean_convariance_matrix):\n",
    "\n",
    "    \"\"\"\n",
    "    A custom metric used to discern the accuracy of the model without influencing\n",
    "    how the models weights and biases are adjusted\n",
    "    \"\"\"\n",
    "    #Determine the number of targets\n",
    "    n_targets = np.shape(true)[1]\n",
    "\n",
    "    #Select the predicted values of the targets\n",
    "    means = mean_convariance_matrix[:, :n_targets]\n",
    "\n",
    "    #Determine the root mean square of the values\n",
    "    diff = tf.math.subtract(true,means)\n",
    "    square = tf.square(diff)\n",
    "    mean_square_error = tf.math.reduce_sum(square)\n",
    "    #Return the accuracy\n",
    "    root_square_error = tf.math.sqrt(mean_square_error)\n",
    "    return root_square_error.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normal_Accuracy_Metric(true,meanscovs_matrix):\n",
    "    \"\"\"\n",
    "    The primary function of the LogNormal loss function is to determine\n",
    "    best normal distribution to fit to the bhadron data. By including the \n",
    "    uncertainity however, the metric is not so usefull for error checking. \n",
    "    I have added accuracy metric to better measure the ability of the neural \n",
    "    network to predict the correct values\n",
    "    \"\"\"\n",
    "    # Determine the number of features we are predicting\n",
    "    n_targets = np.shape(true)[1]\n",
    "    \n",
    "    # Extract the means of the features\n",
    "    means = meanscovs_matrix[:,:n_targets]\n",
    "\n",
    "    Accuracy = []\n",
    "    for n_target in range(n_targets):\n",
    "        Accuracy.append(abs((means[:,n_target]-true[:,n_target])/true[:,n_target])*100)\n",
    "    Accuracy = tf.convert_to_tensor(Accuracy)\n",
    "    return keras.backend.mean(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictOnEpoch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model, x_test, y_test):\n",
    "        self.model = model\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        px_pred = self.model.predict(self.x_test)\n",
    "        Figure = binneddensity(px_pred[0], fixedbinning(0, 100000,1000),label = \"Predicted x momentum values\")\n",
    "        Figure.patch.set_facecolor('white')\n",
    "        Figure.savefig(\"/home/physics/phujdj/DeepLearningParticlePhysics/EpochPlots/PxPredictionOnEpoch-{Epoch}.png\".format(Epoch = epoch),facecolor=Figure.get_facecolor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogNormal_Loss_Function_Check(true,meanscovs_matrix):\n",
    "    \"\"\"The role of this function is to calculate the loss for each individual b jet. This is used for the purpose of error checking\"\"\"\n",
    "    n_targets = np.shape(true)[0]\n",
    "    # Obtain data from convarience matrix\n",
    "    means = meanscovs_matrix[0, :n_targets]\n",
    "    # ensure diagonal is postive:\n",
    "    logsigma = meanscovs_matrix[0, n_targets:2*n_targets]\n",
    "\n",
    "    loss = []\n",
    "    for n_target in range(n_targets):\n",
    "        loss.append(((means[n_target] - true[n_target])**2) / (2 * keras.backend.exp(logsigma[n_target])**2) + logsigma[n_target])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expontial_decay(lr0,s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        if epoch % 100 == 0:\n",
    "            return lr0 * 10\n",
    "        return lr0 * 0.40**(epoch/s)\n",
    "    return exponential_decay_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is being stored in a tree datastructure.\n",
    "# We access the charm root using this command\n",
    "tree = uproot.open(\"hffrag.root:CharmAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "MASKVAL = -999 # This value is introduced to ensure arrays are regular (Of the same size). They will be masked later by the network\n",
    "MAXTRACKS = 32 # This value is the maximum number of tracks allowed per event\n",
    "BATCHSIZE = 64 # This is the batch size of the mini batches used during training\n",
    "EPOCHS = 100 # This is the default number of epochs for which the neural network will train providing that early stopping does not occur\n",
    "MAXEVENTS = 1e20 #This is the maximum number of events that will the program will accept\n",
    "LR = 1e-4 #This is the default learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we wish to study\n",
    "track_features = [\"AnalysisTracks_pt\", \"AnalysisTracks_eta\", \"AnalysisTracks_phi\", \"AnalysisTracks_z0sinTheta\",\n",
    "                  \"AnalysisTracks_d0sig\", \"AnalysisTracks_d0\", \"AnalysisTracks_d0sigPV\", \"AnalysisTracks_d0PV\"]\n",
    "jet_features = [\"AnalysisAntiKt4TruthJets_pt\", \"AnalysisAntiKt4TruthJets_eta\", \"AnalysisAntiKt4TruthJets_phi\",\n",
    "                \"AnalysisAntiKt4TruthJets_ghostB_pt\", \"AnalysisAntiKt4TruthJets_ghostB_eta\",\"AnalysisAntiKt4TruthJets_ghostB_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dat from the root file\n",
    "features = tree.arrays(jet_features+track_features, entry_stop=MAXEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the events of interest\n",
    "events = features[ak.sum(\n",
    "    features[\"AnalysisAntiKt4TruthJets_pt\"] > 25000, axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jets to train on is:  141329\n",
      "The number of track features is:  8\n"
     ]
    }
   ],
   "source": [
    "# Displays the number of jets being trained on\n",
    "jets = events[jet_features][:, 0]\n",
    "print(\"The number of jets to train on is: \", len(jets))\n",
    "print(\"The number of track features is: \",len(track_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tracks from the events\n",
    "tracks = events[track_features]\n",
    "\n",
    "# Match the tracks to the jets\n",
    "matchedtracks = tracks[Match_Tracks(jets, tracks)]\n",
    "\n",
    "# Pad and Flatten the data\n",
    "matchedtracks = flatten(matchedtracks, MAXTRACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 outputs\n",
      "There are 32 inputs\n"
     ]
    }
   ],
   "source": [
    "# Identify the the bottom jets and their associated tracks\n",
    "bjets = ak.sum(jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] > 5000, axis=1) > 0\n",
    "jets = jets[bjets]\n",
    "\n",
    "# Obtain the pt, eta and phi of each b hadron jet\n",
    "bhads_pt = jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"][:, 0].to_numpy()\n",
    "bhads_eta = jets[\"AnalysisAntiKt4TruthJets_ghostB_eta\"][:,0].to_numpy()\n",
    "bhads_phi = jets[\"AnalysisAntiKt4TruthJets_ghostB_phi\"][:,0].to_numpy()\n",
    "\n",
    "bhads = np.stack([bhads_pt,bhads_eta,bhads_phi],axis = -1) #Combine the momentum, eta and phi for each jet into one array\n",
    "\n",
    "print(\"There are {} outputs\".format(np.shape(bhads)[1])) # Display the number of target features the neural network will predict\n",
    "matchedtracks = matchedtracks[bjets]\n",
    "print(\"There are {} inputs\".format(np.shape(matchedtracks)[1])) # Display the number of target features the neural network will use in it's ppredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 3)\n",
      "(5,)\n",
      "[1.48e+05, 1.04e+05, 1.16e+05, 4.03e+04, ... 8.14e+04, 9.83e+04, 1.45e+05, 9.11e+04]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(bhads)) #Check the shape of the neural network\n",
    "print(np.shape(jet_features[:-1])) #Check for shape of the jet features\n",
    "print(jets[jet_features[0]]) # Check the jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 3)\n"
     ]
    }
   ],
   "source": [
    "# Transform the jet and tracks to unstructed data.\n",
    "jets = structured_to_unstructured(jets[jet_features[:-3]])\n",
    "matchedtracks = structured_to_unstructured(matchedtracks)\n",
    "print(np.shape(jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.47e+04, 0.753, 1.14, 1.19, 75.5, ... -0.165, -0.51, -0.0283, -0.692, -0.038]]]\n",
      "(68143, 32)\n"
     ]
    }
   ],
   "source": [
    "#Check the matchtracks are the correct shape\n",
    "print(matchedtracks[:, 0:1])\n",
    "print(np.shape(matchedtracks[:, :, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 32, 3)\n",
      "(68143, 32, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12543/1222727277.py:16: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n"
     ]
    }
   ],
   "source": [
    "# Convert the coordinates of the b jets and tracks to cartesian coordinates\n",
    "tracks_p = pt_eta_phi_2_px_py_pz_tracks(matchedtracks.to_numpy())\n",
    "bhads = pt_eta_phi_2_px_py_pz_jets(bhads)\n",
    "\n",
    "#Check the shape of the momenta of the tracks and the rest of the data is consistent\n",
    "print(np.shape(tracks_p))\n",
    "print(np.shape(matchedtracks[:, :, 3:]))\n",
    "\n",
    "#Combine the momenta of the tracks with the rest of the track features to form the track dataset\n",
    "tracks = np.concatenate([tracks_p,matchedtracks[:,:,3:].to_numpy()],axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 32, 8)\n",
      "(68143, 3)\n",
      "[6.20926450e+03 1.33553447e+04 1.21693980e+04 1.18753994e+00\n",
      " 7.55359192e+01 1.33110714e+00 8.57456207e+01 1.32391548e+00]\n",
      "[ 48855.56531144 128363.19160447 124938.01790683]\n"
     ]
    }
   ],
   "source": [
    "#Check that this is all the correct shape\n",
    "print(np.shape(tracks))\n",
    "print(np.shape(bhads))\n",
    "print(tracks[0,0])\n",
    "print(bhads[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 32, 8)\n",
      "[0.26189855 0.38175348 0.10230412 1.42959932 1.58783932 1.42992229\n",
      " 1.60955267 1.42990682]\n"
     ]
    }
   ],
   "source": [
    "Scaler = StandardScaler()\n",
    "Num_events,Num_tracks,Num_features = np.shape(tracks)\n",
    "tracks = np.reshape(tracks, newshape=(-1,Num_features))\n",
    "tracks = Scaler.fit_transform(tracks)\n",
    "tracks = np.reshape(tracks, newshape= (Num_events,Num_tracks,Num_features))\n",
    "print(np.shape(tracks))\n",
    "print(tracks[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    tracks, bhads, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(sequence_size,d_model,mask_zero=True)\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "    \n",
    "    def call(self,x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_tracks = ObjectEmbedding(sequence_size = np.shape(X_train)[1], d_model = 512)\n",
    "embeded_b_hadrons = ObjectEmbedding(sequence_size=np.shape(y_train)[1], d_model = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(embeded_tracks))\n",
    "print(np.shape(embeded_b_hadrons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    def call(self,x,context):\n",
    "        attn_output, attn_scores = self.mha(query = x, key = context, \n",
    "        value = context,\n",
    "         return_attention_scores = True)\n",
    "    \n",
    "        #Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x,attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query = x,\n",
    "            value = x,\n",
    "            key = x\n",
    "        )\n",
    "        x = self.add([x,attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self,x):\n",
    "        attn_output = self.mha(\n",
    "            query = x,\n",
    "            value = x,\n",
    "            key = x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x,attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model, diff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(diff,activation = 'elu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.dropout(dropout)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.add([x,self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, diff, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim = d_model,\n",
    "            dropout = dropout_rate\n",
    "        )\n",
    "    \n",
    "        self.ffn = FeedForward(d_model, diff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54514, 32, 8)\n",
      "(54514, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 13:01:24.144999: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at mkl_einsum_op.cc:162 : INVALID_ARGUMENT: Expected input 1 to have rank 3 but got: 4\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'multi_head_attention_7' (type MultiHeadAttention).\n\n{{function_node __wrapped____MklEinsum_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected input 1 to have rank 3 but got: 4 [Op:Einsum]\n\nCall arguments received by layer 'multi_head_attention_7' (type MultiHeadAttention):\n  • query=tf.Tensor(shape=(54514, 32, 8), dtype=float32)\n  • value=array([[ -85204.02824177,  -68300.12259415,  112933.04017868],\n       [  21691.09368661,   11842.91751954,   45277.72048297],\n       [  15571.72779763,  -66472.08592353,    6899.34870592],\n       ...,\n       [  63070.6344611 ,   50332.99045299, -146675.46031418],\n       [ -82344.6320273 ,   45736.93034021,  126294.69087274],\n       [ -56077.35474381,  -20784.85892305, -228240.89238799]])\n  • key=array([[ -85204.02824177,  -68300.12259415,  112933.04017868],\n       [  21691.09368661,   11842.91751954,   45277.72048297],\n       [  15571.72779763,  -66472.08592353,    6899.34870592],\n       ...,\n       [  63070.6344611 ,   50332.99045299, -146675.46031418],\n       [ -82344.6320273 ,   45736.93034021,  126294.69087274],\n       [ -56077.35474381,  -20784.85892305, -228240.89238799]])\n  • attention_mask=None\n  • return_attention_scores=True\n  • training=None\n  • use_causal_mask=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12543/2689300844.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_ca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/SpocFit/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12543/1064531157.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCrossAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAttention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         attn_output, attn_scores = self.mha(query = x, key = context, \n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          return_attention_scores = True)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'multi_head_attention_7' (type MultiHeadAttention).\n\n{{function_node __wrapped____MklEinsum_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected input 1 to have rank 3 but got: 4 [Op:Einsum]\n\nCall arguments received by layer 'multi_head_attention_7' (type MultiHeadAttention):\n  • query=tf.Tensor(shape=(54514, 32, 8), dtype=float32)\n  • value=array([[ -85204.02824177,  -68300.12259415,  112933.04017868],\n       [  21691.09368661,   11842.91751954,   45277.72048297],\n       [  15571.72779763,  -66472.08592353,    6899.34870592],\n       ...,\n       [  63070.6344611 ,   50332.99045299, -146675.46031418],\n       [ -82344.6320273 ,   45736.93034021,  126294.69087274],\n       [ -56077.35474381,  -20784.85892305, -228240.89238799]])\n  • key=array([[ -85204.02824177,  -68300.12259415,  112933.04017868],\n       [  21691.09368661,   11842.91751954,   45277.72048297],\n       [  15571.72779763,  -66472.08592353,    6899.34870592],\n       ...,\n       [  63070.6344611 ,   50332.99045299, -146675.46031418],\n       [ -82344.6320273 ,   45736.93034021,  126294.69087274],\n       [ -56077.35474381,  -20784.85892305, -228240.89238799]])\n  • attention_mask=None\n  • return_attention_scores=True\n  • training=None\n  • use_causal_mask=False"
     ]
    }
   ],
   "source": [
    "sample_ca = CrossAttention(num_heads = 2, key_dim = 3)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "\n",
    "\n",
    "print(sample_ca(X_train,y_train).shape())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a05ddcd8ffea9a6a7d2e914b733df5445b717626b5b8c92c04bfc4eb6e7f5cba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
