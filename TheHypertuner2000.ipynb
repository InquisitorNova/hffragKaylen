{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import os\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import numpy as np\n",
    "from Sum import Sum\n",
    "import uproot\n",
    "import nbimporter\n",
    "import import_ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "from hffrag import fixedbinning\n",
    "from hffrag import binneddensity\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import DeepSetNeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is being stored in a tree datastructure. \n",
    "#We access the charm root using this command\n",
    "tree = uproot.open(\"hffrag.root:CharmAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXEVENTS = 1e20\n",
    "MAXTRACKS = 32\n",
    "LR = 1e-2\n",
    "MASKVAL = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data from the root file\n",
    "track_features = [\"AnalysisTracks_pt\",\"AnalysisTracks_eta\",\"AnalysisTracks_phi\",\"AnalysisTracks_z0sinTheta\",\"AnalysisTracks_d0sig\",\"AnalysisTracks_d0\",\"AnalysisTracks_d0sigPV\",\"AnalysisTracks_d0PV\"]\n",
    "jet_features = [\"AnalysisAntiKt4TruthJets_pt\", \"AnalysisAntiKt4TruthJets_eta\", \"AnalysisAntiKt4TruthJets_phi\",\n",
    "                \"AnalysisAntiKt4TruthJets_ghostB_pt\", \"AnalysisAntiKt4TruthJets_ghostB_eta\",\"AnalysisAntiKt4TruthJets_ghostB_phi\"]\n",
    "features = tree.arrays(jet_features+track_features,entry_stop = MAXEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the events of interest\n",
    "events = features[ak.sum(features[\"AnalysisAntiKt4TruthJets_pt\"] > 25000, axis = 1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the number of jets being trained on\n",
    "jets = events[jet_features][:,0]\n",
    "print(\"The number of jets to train on is: \", len(jets))\n",
    "\n",
    "#Select tracks from the events\n",
    "tracks = events[track_features]\n",
    "\n",
    "#Match the tracks to the jets\n",
    "matchedtracks = tracks[DeepSetNeuralNet.Match_Tracks(jets,tracks)]\n",
    "\n",
    "#Pad and Flatten the data\n",
    "matchedtracks = DeepSetNeuralNet.flatten(matchedtracks, MAXTRACKS)\n",
    "\n",
    "# Identify the the bottom jets and their associated tracks\n",
    "bjets = ak.sum(jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] > 5000, axis=1) > 0\n",
    "jets = jets[bjets]\n",
    "bhads_pt = jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"][:, 0].to_numpy()\n",
    "bhads_eta = jets[\"AnalysisAntiKt4TruthJets_ghostB_eta\"][:,0].to_numpy()\n",
    "bhads_phi = jets[\"AnalysisAntiKt4TruthJets_ghostB_phi\"][:,0].to_numpy()\n",
    "bhads = np.stack([bhads_pt,bhads_eta,bhads_phi],axis = -1)\n",
    "\n",
    "print(\"There are {} outputs\".format(np.shape(bhads)[1]))\n",
    "matchedtracks = matchedtracks[bjets]\n",
    "print(\"There are {} inputs\".format(np.shape(matchedtracks)[1]))\n",
    "\n",
    "#Transform the jet and tracks to unstructed data.\n",
    "jets = structured_to_unstructured(jets[jet_features[:-3]])\n",
    "matchedtracks = structured_to_unstructured(matchedtracks)\n",
    "\n",
    "#Fix the angles\n",
    "jets = DeepSetNeuralNet.pt_eta_phi_2_px_py_pz_jets(jets).to_numpy()\n",
    "tracks_p = DeepSetNeuralNet.pt_eta_phi_2_px_py_pz_tracks(matchedtracks.to_numpy())\n",
    "bhads = DeepSetNeuralNet.pt_eta_phi_2_px_py_pz_jets(bhads)\n",
    "print(np.shape(tracks_p))\n",
    "print(np.shape(matchedtracks[:, :, 3:]))\n",
    "tracks = np.concatenate([tracks_p,matchedtracks[:,:,3:].to_numpy()],axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training and validation data\n",
    "X_train = np.load(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/X_train_data\")\n",
    "X_valid = np.load(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/X_valid_data\")\n",
    "y_train = np.load(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/y_train_data\")\n",
    "y_valid = np.load(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/y_valid_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Keras model that is a mirror image of the DeepSetNeuralNetwork to tune the hyperparameters of.\n",
    "def model_builder(hp):\n",
    "    \"\"\"\n",
    "    This function lays out the Deep Set Neural Architecture\n",
    "    - A neural network is applied first to the tracks to extract information from the tracks.\n",
    "    - This information produces an ensemble space which, the outputs of which are then summed to produce\n",
    "        the inputs for the next layer\n",
    "    - A neural network is then applied to the jet data obtained from the tracks. \n",
    "        To perform current univariate regression.\n",
    "    \"\"\"\n",
    "    # Create the ranges of hyperparameters to explore\n",
    "    dropouts = hp.Choice('dropout', [0.001,0.05,0.20,0.40,0.50,0.60,0.70])\n",
    "    track_layer = hp.Choice('track_layers',[32,64,128,256,512])\n",
    "    jet_layer = hp.Choice('jet_layers',[32,64,128,256,512])\n",
    "    activation_func = hp.Choice('act_func',[\"relu\",\"elu\",\"selu\"])\n",
    "    Learning_rate = hp.Choice('learning_rate',[1e-6,1e-5,1e-4,1e-3,1e-2])\n",
    "    regularizers = hp.Choice(\"regularizer\", [1e-6,1e-5,1e-4,1e-3,1e-2,1e-1])\n",
    "    dropout_frequency = hp.Choice(\"DropoutFrequency\",[1,2,3,4])\n",
    "\n",
    "    #Create the track and jet layers\n",
    "    track_layers = [len(track_features)]+[track_layer,track_layer,track_layer,track_layer,track_layer]\n",
    "    jet_layers = [jet_layer,jet_layer,jet_layer,jet_layer,jet_layer,jet_layer]\n",
    "\n",
    "    #Set the number of targets being explored\n",
    "    n_targets = 3\n",
    "    \n",
    "    #Follows the DeepSetNeural Architecture\n",
    "    inputs = layers.Input(shape=(None, track_layers[0])) # Creates a layer for each input\n",
    "    outputs = inputs  # Creates another layer to pass the inputs onto the ouputs\n",
    "    outputs = layers.Masking(mask_value=MASKVAL)(outputs) # Masks the MASKVAl values\n",
    "\n",
    "    counter = 0\n",
    "    for nodes in track_layers[:-1]:\n",
    "        #The first neural network is a series of dense layers and is applied to each track using the time distributed layer\n",
    "        outputs = layers.TimeDistributed( \n",
    "            layers.Dense(nodes, activation=activation_func, kernel_initializer= \"he_normal\",kernel_regularizer = keras.regularizers.l2(regularizers)))(outputs) # We use relu and the corresponding he_normal for the activation function and bias initializer\n",
    "        if counter % dropout_frequency == 0: # Every two layers apply a dropout\n",
    "            outputs = layers.Dropout(dropouts)(outputs)\n",
    "        else:\n",
    "            counter += 1\n",
    "        outputs = layers.BatchNormalization()(outputs) #Apply a batch norm to improve performance by preventing feature bias and overfitting\n",
    "\n",
    "    outputs = layers.TimeDistributed(layers.Dense( \n",
    "        track_layers[-1], activation='softmax'))(outputs) # Apply softmax to ouput the results of the track neural network as probabilities\n",
    "    outputs = Sum()(outputs) # Sum the outputs to make use of permutation invariance\n",
    "\n",
    "    counter = 0\n",
    "    for nodes in jet_layers: #Repeat of the track neural network without the need for the timedistributed layers\n",
    "        outputs = layers.Dense(nodes, activation=activation_func, kernel_initializer= \"he_normal\",kernel_regularizer = keras.regularizers.l2(regularizers))(outputs)\n",
    "        if counter % dropout_frequency == 0:\n",
    "            outputs = layers.Dropout(dropouts)(outputs)\n",
    "        else:\n",
    "            counter += 1\n",
    "        outputs = layers.BatchNormalization()(outputs)\n",
    "\n",
    "    outputs = layers.Dense(n_targets+n_targets*(n_targets+1)//2)(outputs) # The output will have a number of neurons needed to form the mean covariance function of the loss func\n",
    "\n",
    "    Model = keras.Model(inputs=inputs, outputs=outputs) #Create a keras model\n",
    "\n",
    "    # Specify the neural network's optimizer and loss function\n",
    "    Model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(learning_rate=Learning_rate,clipnorm = 1.0), # Optimizer used to train model\n",
    "    metrics = [DeepSetNeuralNet.Normal_Accuracy_Metric], # Metric used to assess true performance of model\n",
    "    loss=DeepSetNeuralNet.LogNormal_Loss_Function, #Loss function\n",
    "    )\n",
    "\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the hyperparameter\n",
    "SEED = tf.random.set_seed(42) # Generate a random seed\n",
    "max_trials = 15 # Set the number of trials\n",
    "tuner = kt.RandomSearch(model_builder,\n",
    "                        objective='val_loss',\n",
    "                        seed=SEED,\n",
    "                        overwrite=True,\n",
    "                        max_trials=max_trials,\n",
    "                        directory='/home/physics/phujdj/DeepLearningParticlePhysics',\n",
    "                        project_name=\"DeepSetHyperTraining\",\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an early stoping to properly survey the values\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the parameter space to obtain the best hyperparameter values\n",
    "tuner.search(X_train, y_train, validation_data=(\n",
    "    X_valid, y_valid), epochs=20, callbacks=[stop_early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=10)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of track layers is {best_hps.get('track_layers')}, the optimal number of jet layers is {best_hps.get('jet_layers')}, the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}, the optimal dropout rate is {best_hps.get('dropout')} and finally the optimal activation function is {best_hps.get('act_func')}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('DeepLearningParticlePhysics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eadd9fb5f4dc5349e458d3ad6d3a5847384a7f4d0db1c8d129ea191670ee9b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
