{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf52dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 17:39:28.925237: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 17:39:29.080533: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-23 17:39:29.086143: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-23 17:39:29.086156: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-23 17:39:29.116984: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 17:39:30.295231: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 17:39:30.295308: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 17:39:30.295323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PDG ID: 531\n",
      "number of b-hadrons: 13487\n",
      "\n",
      "PDG ID: -521\n",
      "number of b-hadrons: 61531\n",
      "\n",
      "PDG ID: -511\n",
      "number of b-hadrons: 61243\n",
      "\n",
      "PDG ID: 5232\n",
      "number of b-hadrons: 751\n",
      "\n",
      "PDG ID: 511\n",
      "number of b-hadrons: 61706\n",
      "\n",
      "PDG ID: 521\n",
      "number of b-hadrons: 61594\n",
      "\n",
      "PDG ID: -531\n",
      "number of b-hadrons: 13526\n",
      "\n",
      "PDG ID: -5122\n",
      "number of b-hadrons: 5447\n",
      "\n",
      "PDG ID: 5122\n",
      "number of b-hadrons: 5232\n",
      "\n",
      "PDG ID: -5132\n",
      "number of b-hadrons: 676\n",
      "\n",
      "PDG ID: 5132\n",
      "number of b-hadrons: 722\n",
      "\n",
      "PDG ID: -5232\n",
      "number of b-hadrons: 733\n",
      "\n",
      "PDG ID: 555\n",
      "number of b-hadrons: 3\n",
      "\n",
      "PDG ID: 553\n",
      "number of b-hadrons: 6\n",
      "\n",
      "PDG ID: -5332\n",
      "number of b-hadrons: 21\n",
      "\n",
      "PDG ID: 5332\n",
      "number of b-hadrons: 18\n",
      "\n",
      "PDG ID: 100553\n",
      "number of b-hadrons: 1\n",
      "\n",
      "PDG ID: -541\n",
      "number of b-hadrons: 5\n",
      "\n",
      "PDG ID: 10551\n",
      "number of b-hadrons: 3\n",
      "\n",
      "PDG ID: 541\n",
      "number of b-hadrons: 5\n",
      "\n",
      "PDG ID: 20553\n",
      "number of b-hadrons: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import relevant modules\n",
    "import os\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "from keras import callbacks\n",
    "import keras\n",
    "import uproot\n",
    "from Sum import Sum\n",
    "import sklearn as sk\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import hffrag\n",
    "import keras_tuner as kt\n",
    "from hffrag import fixedbinning\n",
    "from hffrag import binneddensity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03fbe6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A magic operator to allow Jupyter Notebooks to display matplotlib plots as outputs of cells\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646b0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is being stored in a tree datastructure.\n",
    "# We access the charm root using this command\n",
    "tree = uproot.open(\"hffrag.root:CharmAnalysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85a8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "MASKVAL = -999 # This value is introduced to ensure arrays are regular (Of the same size). They will be masked later by the network\n",
    "MAXTRACKS = 32 # This value is the maximum number of tracks allowed per event\n",
    "BATCHSIZE = 64 # This is the batch size of the mini batches used during training\n",
    "EPOCHS = 1000 # This is the default number of epochs for which the neural network will train providing that early stopping does not occur\n",
    "MAXEVENTS = 1e20 #This is the maximum number of events that will the program will accept\n",
    "LR = 1e-2 #This is the default learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10435331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the associated tracks for each jet\n",
    "def Match_Tracks(jets, tracks):\n",
    "    \"\"\"Used to determine if a set of tracks belong to a particular set of jets\"\"\"\n",
    "\n",
    "    jet_eta = jets[\"AnalysisAntiKt4TruthJets_eta\"]\n",
    "    jet_phi = jets[\"AnalysisAntiKt4TruthJets_phi\"] \n",
    "\n",
    "    tracks_eta = tracks[\"AnalysisTracks_eta\"]\n",
    "    tracks_phi = tracks[\"AnalysisTracks_phi\"]\n",
    "\n",
    "    delta_etas = jet_eta - tracks_eta\n",
    "    delta_phis = np.abs(jet_phi - tracks_phi)\n",
    "\n",
    "    # Map the phis from a cyclical period onto a linear relation\n",
    "    ak.where(delta_phis > np.pi, delta_phis - np.pi, delta_phis)\n",
    "\n",
    "    # Returns a list of true and false, determining which tracks belong to those jets.\n",
    "    return np.sqrt(delta_phis**2 + delta_etas**2) < 0.4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f5e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from cylindrical to cartesian coordinates\n",
    "def pt_eta_phi_2_px_py_pz_jets(pt_eta_phi):\n",
    "    \"\"\"Converts the cylindrical polar coordinates to cartesian coordinates for jets\"\"\"\n",
    "\n",
    "    # Seperate the pts, etas and phis\n",
    "    pts = pt_eta_phi[:, 0:1]\n",
    "    etas = pt_eta_phi[:, 1:2]\n",
    "    phis = pt_eta_phi[:, 2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    pxs = pts * np.cos(phis)\n",
    "    pys = pts * np.sin(phis)\n",
    "    pzs = pts * np.sinh(etas)\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector\n",
    "    return np.concatenate([pxs, pys, pzs], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31bd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_eta_phi_2_px_py_pz_tracks(pt_eta_phi, MASKVAL=-999):\n",
    "    \"\"\"Converts the cylindrical polar coordinates to cartesian coordinates for jets\"\"\"\n",
    "\n",
    "    # Seperate the pts, etas and phis\n",
    "    pts = pt_eta_phi[:, :, 0:1]\n",
    "    etas = pt_eta_phi[:, :, 1:2]\n",
    "    phis = pt_eta_phi[:, :, 2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    # Transforms only the non masked values from cylindrical to cartesian coordinates. Mask values are left unchanged.\n",
    "    mask1 = pts == MASKVAL \n",
    "    mask2 = phis == MASKVAL\n",
    "    mask3 = etas == MASKVAL\n",
    "    pxs = np.where(mask1 | mask2, pts, pts * np.cos(phis)) \n",
    "    pys = np.where(mask1 | mask2, pts, pts * np.sin(phis))\n",
    "    pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector in cartesian coordinates\n",
    "    return np.concatenate([pxs, pys, pzs], axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a032c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_eta_phi2_px_py_pz_predicted_tracks(predictions):\n",
    "    #Obtain the pts,etas and phis\n",
    "    pts = predictions[:,0:1]\n",
    "    etas = predictions[:,1:2]\n",
    "    phis = predictions[:,2:3]\n",
    "\n",
    "    # Convert from polar to cartesian\n",
    "    # Transforms only the non masked values from cylindrical to cartesian coordinates. Mask values are left unchanged.\n",
    "    pxs =  pts * np.cos(phis)\n",
    "    pys =  pts * np.sin(phis)\n",
    "    pzs =  pts * np.sinh(etas)\n",
    "\n",
    "    # Check to see if there are any infinities\n",
    "    isinf = np.isinf(pzs)\n",
    "\n",
    "    if np.any(isinf):\n",
    "        print(\"Infinities in eta detected!\")\n",
    "        print(etas[isinf])\n",
    "        raise ValueError(\"Infinity from sinh(eta) has been detected\")\n",
    "\n",
    "    # Returns the momentum vector in cartesian coordinates\n",
    "    return np.concatenate([pxs, pys, pzs], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd743c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x_values, maxsize, MASKVAL=-999):\n",
    "    \"\"\"\n",
    "    Pads the inputs with nans to get to the maxsize\n",
    "    \"\"\"\n",
    "    #Pad the non-regular arrays with null values until they are all of the same size. Then replace the nulls with MASVAL\n",
    "    y_values = ak.fill_none(ak.pad_none(x_values, maxsize, axis=1, clip=True), MASKVAL)[:, :maxsize]\n",
    "    return ak.to_regular(y_values, axis=1) #Return the regular arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849df671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x_values, maxsize=-1, MASKVAL=-999):\n",
    "    \"\"\"\"Pads the input to ensure they are all of regular size and then zips together result\"\"\"\n",
    "    y_values = {} \n",
    "    for field in x_values.fields:\n",
    "        z_values = x_values[field]\n",
    "        if maxsize > 0:\n",
    "            z_values = pad(z_values, maxsize, MASKVAL)\n",
    "        y_values[field] = z_values\n",
    "\n",
    "    return ak.zip(y_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f51140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogNormal_Loss_Function(true, meanscovs_matrix):\n",
    "    \"\"\"\n",
    "    This is a loss function hand crafted for the task of ensuring the neural network \n",
    "    learns to predict the true value of the transverse momentum and it's uncertainty\n",
    "    The logNormal constrains the neural network, by forcing upon it what it's output layers should be\n",
    "    and what the weights and biases of the neural network will be in order to predict the means, variances and covariances\n",
    "    \"\"\"\n",
    "    n_targets = np.shape(true)[1]\n",
    "    # The first n_target of the features are the means\n",
    "    means = meanscovs_matrix[:, :n_targets]\n",
    "    # The second n_target of the feautres are the standard deviations\n",
    "    logsigma = meanscovs_matrix[:, n_targets:2*n_targets]\n",
    "    # The rest of the targets are the covariances\n",
    "    logcosigma = meanscovs_matrix[:,2*n_targets:]\n",
    "\n",
    "    loss = 0\n",
    "    for n_target in range(n_targets): #Sum the individual losses and use that as the loss for the neural network\n",
    "        loss += ((means[:, n_target] - true[:, n_target])**2) / (2 * keras.backend.exp(logsigma[:, n_target])**2) + logsigma[:, n_target]\n",
    "\n",
    "    # Build loss function\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abab3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normal_Accuracy_Metric(true,meanscovs_matrix):\n",
    "    \"\"\"\n",
    "    The primary function of the LogNormal loss function is to determine\n",
    "    best normal distribution to fit to the bhadron data. By including the \n",
    "    uncertainity however, the metric is not so usefull for error checking. \n",
    "    I have added accuracy metric to better measure the ability of the neural \n",
    "    network to predict the correct values\n",
    "    \"\"\"\n",
    "    # Determine the number of features we are predicting\n",
    "    n_targets = np.shape(true)[1]\n",
    "    \n",
    "    # Extract the means of the features\n",
    "    means = meanscovs_matrix[:,:n_targets]\n",
    "\n",
    "    Accuracy = []\n",
    "    for n_target in range(n_targets):\n",
    "        Accuracy.append(abs((means[:,n_target]-true[:,n_target])/true[:,n_target])*100)\n",
    "    Accuracy = tf.convert_to_tensor(Accuracy)\n",
    "    return keras.backend.mean(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51140913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogNormal_Loss_Function_Check(true,meanscovs_matrix):\n",
    "    \"\"\"The role of this function is to calculate the loss for each individual b jet. This is used for the purpose of error checking\"\"\"\n",
    "    n_targets = np.shape(true)[0]\n",
    "    # Obtain data from convarience matrix\n",
    "    means = meanscovs_matrix[0, :n_targets]\n",
    "    # ensure diagonal is postive:\n",
    "    logsigma = meanscovs_matrix[0, n_targets:2*n_targets]\n",
    "\n",
    "    loss = []\n",
    "    for n_target in range(n_targets):\n",
    "        loss.append(((means[n_target] - true[n_target])**2) / (2 * keras.backend.exp(logsigma[n_target])**2) + logsigma[n_target])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb387c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expontial_decay(lr0,s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "exponential_decay_fn = expontial_decay(lr0 = LR,s = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6ecfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepSetNeuralNetwork(track_layers, jet_layers, n_targets,Learning_rate, MASKVAL=-999):\n",
    "    \"\"\"\n",
    "    This function lays out the Deep Set Neural Architecture\n",
    "    - A neural network is applied first to the tracks to extract information from the tracks.\n",
    "    - This information produces an ensemble space which, the outputs of which are then summed to produce\n",
    "        the inputs for the next layer\n",
    "    - A neural network is then applied to the jet data obtained from the tracks. \n",
    "        To perform current univariate regression.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(None, track_layers[0])) # Creates a layer for each input\n",
    "    outputs = inputs  # Creates another layer to pass the inputs onto the ouputs\n",
    "    outputs = layers.Masking(mask_value=MASKVAL)(outputs) # Masks the MASKVAl values\n",
    "\n",
    "    counter = 0\n",
    "    for nodes in track_layers[:-1]:\n",
    "        #The first neural network is a series of dense layers and is applied to each track using the time distributed layer\n",
    "        outputs = layers.TimeDistributed( \n",
    "            layers.Dense(nodes, activation=\"elu\", kernel_initializer= \"he_normal\",kernel_regularizer = keras.regularizers.l2(0.01)))(outputs) # We use relu and the corresponding he_normal for the activation function and bias initializer\n",
    "        if counter % 2 == 0: # Every two layers apply a dropout\n",
    "            outputs = layers.Dropout(0.2)(outputs)\n",
    "        else:\n",
    "            counter += 1\n",
    "        outputs = layers.BatchNormalization()(outputs) #Apply a batch norm to improve performance by preventing feature bias and overfitting\n",
    "\n",
    "    outputs = layers.TimeDistributed(layers.Dense( \n",
    "        track_layers[-1], activation='softmax'))(outputs) # Apply softmax to ouput the results of the track neural network as probabilities\n",
    "    outputs = Sum()(outputs) # Sum the outputs to make use of permutation invariance\n",
    "\n",
    "    counter = 0\n",
    "    for nodes in jet_layers: #Repeat of the track neural network without the need for the timedistributed layers\n",
    "        outputs = layers.Dense(nodes, activation='elu', kernel_initializer= \"he_normal\",kernel_regularizer = keras.regularizers.l2(0.01))(outputs)\n",
    "        if counter % 2 == 0:\n",
    "            outputs = layers.Dropout(0.2)(outputs)\n",
    "        else:\n",
    "            counter += 1\n",
    "        outputs = layers.BatchNormalization()(outputs)\n",
    "\n",
    "    outputs = layers.Dense(n_targets+n_targets*(n_targets+1)//2)(outputs) # The output will have a number of neurons needed to form the mean covariance function of the loss func\n",
    "\n",
    "    Model = keras.Model(inputs=inputs, outputs=outputs) #Create a keras model\n",
    "\n",
    "    # Specify the neural network's optimizer and loss function\n",
    "    Model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(learning_rate=Learning_rate,clipnorm = 1.0), # Optimizer used to train model\n",
    "    metrics = [Normal_Accuracy_Metric], # Metric used to assess true performance of model\n",
    "    loss=LogNormal_Loss_Function, #Loss function\n",
    "    )\n",
    "\n",
    "    return Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09cc7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we wish to study\n",
    "track_features = [\"AnalysisTracks_pt\", \"AnalysisTracks_eta\", \"AnalysisTracks_phi\", \"AnalysisTracks_z0sinTheta\",\n",
    "                  \"AnalysisTracks_d0sig\", \"AnalysisTracks_d0\", \"AnalysisTracks_d0sigPV\", \"AnalysisTracks_d0PV\"]\n",
    "jet_features = [\"AnalysisAntiKt4TruthJets_pt\", \"AnalysisAntiKt4TruthJets_eta\", \"AnalysisAntiKt4TruthJets_phi\",\n",
    "                \"AnalysisAntiKt4TruthJets_ghostB_pt\", \"AnalysisAntiKt4TruthJets_ghostB_eta\",\"AnalysisAntiKt4TruthJets_ghostB_phi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94fb5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the root file\n",
    "features = tree.arrays(jet_features+track_features, entry_stop=MAXEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the events of interest\n",
    "events = features[ak.sum(\n",
    "    features[\"AnalysisAntiKt4TruthJets_pt\"] > 25000, axis=1) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jets to train on is:  141329\n",
      "The number of track features is:  8\n"
     ]
    }
   ],
   "source": [
    "# Displays the number of jets being trained on\n",
    "jets = events[jet_features][:, 0]\n",
    "print(\"The number of jets to train on is: \", len(jets))\n",
    "print(\"The number of track features is: \",len(track_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tracks from the events\n",
    "tracks = events[track_features]\n",
    "\n",
    "# Match the tracks to the jets\n",
    "matchedtracks = tracks[Match_Tracks(jets, tracks)]\n",
    "\n",
    "# Pad and Flatten the data\n",
    "matchedtracks = flatten(matchedtracks, MAXTRACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 outputs\n",
      "There are 32 inputs\n"
     ]
    }
   ],
   "source": [
    "# Identify the the bottom jets and their associated tracks\n",
    "bjets = ak.sum(jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] > 5000, axis=1) > 0\n",
    "jets = jets[bjets]\n",
    "\n",
    "# Obtain the pt, eta and phi of each b hadron jet\n",
    "bhads_pt = jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"][:, 0].to_numpy()\n",
    "bhads_eta = jets[\"AnalysisAntiKt4TruthJets_ghostB_eta\"][:,0].to_numpy()\n",
    "bhads_phi = jets[\"AnalysisAntiKt4TruthJets_ghostB_phi\"][:,0].to_numpy()\n",
    "\n",
    "bhads = np.stack([bhads_pt,bhads_eta,bhads_phi],axis = -1) #Combine the momentum, eta and phi for each jet into one array\n",
    "\n",
    "print(\"There are {} outputs\".format(np.shape(bhads)[1])) # Display the number of target features the neural network will predict\n",
    "matchedtracks = matchedtracks[bjets]\n",
    "print(\"There are {} inputs\".format(np.shape(matchedtracks)[1])) # Display the number of target features the neural network will use in it's ppredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 3)\n",
      "(5,)\n",
      "[1.48e+05, 1.04e+05, 1.16e+05, 4.03e+04, ... 8.14e+04, 9.83e+04, 1.45e+05, 9.11e+04]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(bhads)) #Check the shape of the neural network\n",
    "print(np.shape(jet_features[:-1])) #Check for shape of the jet features\n",
    "print(jets[jet_features[0]]) # Check the jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 3)\n"
     ]
    }
   ],
   "source": [
    "# Transform the jet and tracks to unstructed data.\n",
    "jets = structured_to_unstructured(jets[jet_features[:-3]])\n",
    "matchedtracks = structured_to_unstructured(matchedtracks)\n",
    "print(np.shape(jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.47e+04, 0.753, 1.14, 1.19, 75.5, ... -0.165, -0.51, -0.0283, -0.692, -0.038]]]\n",
      "(68143, 32)\n"
     ]
    }
   ],
   "source": [
    "#Check the matchtracks are the correct shape\n",
    "print(matchedtracks[:, 0:1])\n",
    "print(np.shape(matchedtracks[:, :, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 32, 3)\n",
      "(68143, 32, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19841/1222727277.py:16: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n"
     ]
    }
   ],
   "source": [
    "# Convert the coordinates of the b jets and tracks to cartesian coordinates\n",
    "tracks_p = pt_eta_phi_2_px_py_pz_tracks(matchedtracks.to_numpy())\n",
    "#bhads = pt_eta_phi_2_px_py_pz_jets(bhads)\n",
    "\n",
    "#Check the shape of the momenta of the tracks and the rest of the data is consistent\n",
    "print(np.shape(tracks_p))\n",
    "print(np.shape(matchedtracks[:, :, 3:]))\n",
    "\n",
    "#Combine the momenta of the tracks with the rest of the track features to form the track dataset\n",
    "tracks = np.concatenate([tracks_p,matchedtracks[:,:,3:].to_numpy()],axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 32, 8)\n",
      "[6.20926450e+03 1.33553447e+04 1.21693980e+04 1.18753994e+00\n",
      " 7.55359192e+01 1.33110714e+00 8.57456207e+01 1.32391548e+00]\n",
      "[1.37346188e+05 8.16028237e-01 1.20712149e+00]\n"
     ]
    }
   ],
   "source": [
    "#Check that this is all the correct shape\n",
    "print(np.shape(tracks))\n",
    "print(tracks[0,0])\n",
    "print(bhads[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 17:41:16.302760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-23 17:41:16.302789: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-23 17:41:16.302805: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vonneumann.csc.warwick.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2022-11-23 17:41:16.303026: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Builds the deep neural network\n",
    "track_layers = [32,32,32,32,32]\n",
    "jet_layers = [64,64,64,64,64]\n",
    "DeepNet = DeepSetNeuralNetwork(\n",
    "    [len(track_features)]+track_layers, jet_layers,3, LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 8)]         0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 8)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 8)          72        \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 8)           0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, None, 8)          32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 32)         288       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 32)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, None, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, None, 32)         1056      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 32)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, None, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, None, 32)         1056      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 32)          0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, None, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, None, 32)         1056      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 32)          0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, None, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, None, 32)         1056      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " sum (Sum)                   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 9)                 585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,745\n",
      "Trainable params: 24,833\n",
      "Non-trainable params: 912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Summarises the Neural Network Architecture\n",
    "DeepNet.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    tracks, bhads, train_size=0.7, random_state=42)\n",
    "#Save the training and validation datasets.\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/X_train_data.npy\",X_train)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/X_valid_data.npy\",X_valid)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/y_train_data.npy\",y_train)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/y_valid_data.npy\",y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20443, 32, 8) (20443, 3)\n"
     ]
    }
   ],
   "source": [
    "#Check for the of the training and validation sets\n",
    "print(np.shape(X_valid), np.shape(y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce early_stopping to prevent overfitting\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001,  # The minimum amount of change to count as an improvement\n",
    "    patience=15,  # The number of epochs to wait before stopping\n",
    "    restore_best_weights=True,  # Keep the best weights\n",
    ")\n",
    "# Prevent spikes in the validation and training loss due to the gradient descent kicking the network out of a local minima\n",
    "reduce_learn_on_plateau = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.8, patience=20, min_lr=1e-6)\n",
    "\n",
    "# Save the weights of the model to allow reuse in future.\n",
    "path = \"/home/physics/phujdj/DeepLearningParticlePhysics/CheckPoints/DeepNetWeights&Biases.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                 save_weights_only=True, verbose=0, save_best_only=True)\n",
    "\n",
    "# Learning Scheduler:\n",
    "learning_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "746/746 [==============================] - 12s 10ms/step - loss: 490738496.0000 - Normal_Accuracy_Metric: 248.5264 - val_loss: 15.7631 - val_Normal_Accuracy_Metric: 191.3133 - lr: 0.0100\n",
      "Epoch 2/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 12.8944 - Normal_Accuracy_Metric: 259.9258 - val_loss: 12.9418 - val_Normal_Accuracy_Metric: 158.4404 - lr: 0.0089\n",
      "Epoch 3/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 12.6904 - Normal_Accuracy_Metric: 186.9597 - val_loss: 13.5039 - val_Normal_Accuracy_Metric: 184.7588 - lr: 0.0079\n",
      "Epoch 4/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 12.5320 - Normal_Accuracy_Metric: 200.5511 - val_loss: 14.2408 - val_Normal_Accuracy_Metric: 210.3255 - lr: 0.0071\n",
      "Epoch 5/1000\n",
      "746/746 [==============================] - 6s 8ms/step - loss: 12.4041 - Normal_Accuracy_Metric: 219.2548 - val_loss: 18.3685 - val_Normal_Accuracy_Metric: 241.9577 - lr: 0.0063\n",
      "Epoch 6/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 12.2661 - Normal_Accuracy_Metric: 230.6248 - val_loss: 14.0564 - val_Normal_Accuracy_Metric: 192.0106 - lr: 0.0056\n",
      "Epoch 7/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 12.1718 - Normal_Accuracy_Metric: 153.5417 - val_loss: 13.4267 - val_Normal_Accuracy_Metric: 169.9222 - lr: 0.0050\n",
      "Epoch 8/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 12.1055 - Normal_Accuracy_Metric: 194.1579 - val_loss: 11.9815 - val_Normal_Accuracy_Metric: 103.9837 - lr: 0.0045\n",
      "Epoch 9/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 12.0135 - Normal_Accuracy_Metric: 153.3816 - val_loss: 15.7749 - val_Normal_Accuracy_Metric: 159.5355 - lr: 0.0040\n",
      "Epoch 10/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.9280 - Normal_Accuracy_Metric: 132.6087 - val_loss: 17.2212 - val_Normal_Accuracy_Metric: 220.1383 - lr: 0.0035\n",
      "Epoch 11/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.8955 - Normal_Accuracy_Metric: 132.3314 - val_loss: 12.7558 - val_Normal_Accuracy_Metric: 175.1336 - lr: 0.0032\n",
      "Epoch 12/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.8498 - Normal_Accuracy_Metric: 151.0682 - val_loss: 13.6792 - val_Normal_Accuracy_Metric: 162.6228 - lr: 0.0028\n",
      "Epoch 13/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.8053 - Normal_Accuracy_Metric: 147.8875 - val_loss: 11.6413 - val_Normal_Accuracy_Metric: 95.8491 - lr: 0.0025\n",
      "Epoch 14/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.7891 - Normal_Accuracy_Metric: 137.5409 - val_loss: 19.4083 - val_Normal_Accuracy_Metric: 189.3953 - lr: 0.0022\n",
      "Epoch 15/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.7493 - Normal_Accuracy_Metric: 172.8372 - val_loss: 21.3947 - val_Normal_Accuracy_Metric: 238.0969 - lr: 0.0020\n",
      "Epoch 16/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.7069 - Normal_Accuracy_Metric: 135.0468 - val_loss: 15.7024 - val_Normal_Accuracy_Metric: 225.6085 - lr: 0.0018\n",
      "Epoch 17/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.6898 - Normal_Accuracy_Metric: 119.3864 - val_loss: 12.5967 - val_Normal_Accuracy_Metric: 130.5441 - lr: 0.0016\n",
      "Epoch 18/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.6533 - Normal_Accuracy_Metric: 140.2037 - val_loss: 17.6735 - val_Normal_Accuracy_Metric: 209.1692 - lr: 0.0014\n",
      "Epoch 19/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.6596 - Normal_Accuracy_Metric: 126.1040 - val_loss: 15.4120 - val_Normal_Accuracy_Metric: 138.6956 - lr: 0.0013\n",
      "Epoch 20/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.6374 - Normal_Accuracy_Metric: 135.7032 - val_loss: 20.7765 - val_Normal_Accuracy_Metric: 262.9340 - lr: 0.0011\n",
      "Epoch 21/1000\n",
      "746/746 [==============================] - 6s 8ms/step - loss: 11.6253 - Normal_Accuracy_Metric: 182.2633 - val_loss: 13.0059 - val_Normal_Accuracy_Metric: 141.9120 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.6086 - Normal_Accuracy_Metric: 119.0840 - val_loss: 17.9611 - val_Normal_Accuracy_Metric: 201.4433 - lr: 8.9125e-04\n",
      "Epoch 23/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.5747 - Normal_Accuracy_Metric: 187.7585 - val_loss: 11.5859 - val_Normal_Accuracy_Metric: 79.9800 - lr: 7.9433e-04\n",
      "Epoch 24/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.5686 - Normal_Accuracy_Metric: 131.3973 - val_loss: 21.4213 - val_Normal_Accuracy_Metric: 192.1361 - lr: 7.0795e-04\n",
      "Epoch 25/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.5512 - Normal_Accuracy_Metric: 137.8686 - val_loss: 16.7653 - val_Normal_Accuracy_Metric: 214.9272 - lr: 6.3096e-04\n",
      "Epoch 26/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.5492 - Normal_Accuracy_Metric: 145.7338 - val_loss: 14.0340 - val_Normal_Accuracy_Metric: 164.0051 - lr: 5.6234e-04\n",
      "Epoch 27/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.5315 - Normal_Accuracy_Metric: 141.1947 - val_loss: 18.1365 - val_Normal_Accuracy_Metric: 203.5947 - lr: 5.0119e-04\n",
      "Epoch 28/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.5289 - Normal_Accuracy_Metric: 146.2381 - val_loss: 12.1023 - val_Normal_Accuracy_Metric: 130.9936 - lr: 4.4668e-04\n",
      "Epoch 29/1000\n",
      "746/746 [==============================] - 6s 8ms/step - loss: 11.5344 - Normal_Accuracy_Metric: 147.8339 - val_loss: 26.5670 - val_Normal_Accuracy_Metric: 290.8718 - lr: 3.9811e-04\n",
      "Epoch 30/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4767 - Normal_Accuracy_Metric: 132.5620 - val_loss: 15.7646 - val_Normal_Accuracy_Metric: 171.6196 - lr: 3.5481e-04\n",
      "Epoch 31/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.4922 - Normal_Accuracy_Metric: 128.2477 - val_loss: 12.3880 - val_Normal_Accuracy_Metric: 150.7182 - lr: 3.1623e-04\n",
      "Epoch 32/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4944 - Normal_Accuracy_Metric: 123.7659 - val_loss: 11.4197 - val_Normal_Accuracy_Metric: 95.9520 - lr: 2.8184e-04\n",
      "Epoch 33/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4958 - Normal_Accuracy_Metric: 152.1278 - val_loss: 15.8251 - val_Normal_Accuracy_Metric: 201.4266 - lr: 2.5119e-04\n",
      "Epoch 34/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4497 - Normal_Accuracy_Metric: 124.0619 - val_loss: 13.4638 - val_Normal_Accuracy_Metric: 126.8528 - lr: 2.2387e-04\n",
      "Epoch 35/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4697 - Normal_Accuracy_Metric: 154.1258 - val_loss: 14.3612 - val_Normal_Accuracy_Metric: 151.1734 - lr: 1.9953e-04\n",
      "Epoch 36/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.4540 - Normal_Accuracy_Metric: 149.6889 - val_loss: 20.3440 - val_Normal_Accuracy_Metric: 250.9095 - lr: 1.7783e-04\n",
      "Epoch 37/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.4616 - Normal_Accuracy_Metric: 139.2155 - val_loss: 12.5023 - val_Normal_Accuracy_Metric: 144.4801 - lr: 1.5849e-04\n",
      "Epoch 38/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4347 - Normal_Accuracy_Metric: 116.7234 - val_loss: 19.9292 - val_Normal_Accuracy_Metric: 247.7806 - lr: 1.4125e-04\n",
      "Epoch 39/1000\n",
      "746/746 [==============================] - 6s 8ms/step - loss: 11.4510 - Normal_Accuracy_Metric: 144.0676 - val_loss: 19.6036 - val_Normal_Accuracy_Metric: 212.7849 - lr: 1.2589e-04\n",
      "Epoch 40/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.4495 - Normal_Accuracy_Metric: 128.3382 - val_loss: 11.6767 - val_Normal_Accuracy_Metric: 132.8195 - lr: 1.1220e-04\n",
      "Epoch 41/1000\n",
      "746/746 [==============================] - 6s 8ms/step - loss: 11.4491 - Normal_Accuracy_Metric: 126.2264 - val_loss: 12.8437 - val_Normal_Accuracy_Metric: 184.4155 - lr: 1.0000e-04\n",
      "Epoch 42/1000\n",
      "746/746 [==============================] - 6s 8ms/step - loss: 11.4503 - Normal_Accuracy_Metric: 147.4315 - val_loss: 13.1223 - val_Normal_Accuracy_Metric: 141.2660 - lr: 8.9125e-05\n",
      "Epoch 43/1000\n",
      "746/746 [==============================] - 6s 8ms/step - loss: 11.4188 - Normal_Accuracy_Metric: 114.3606 - val_loss: 16.7828 - val_Normal_Accuracy_Metric: 169.1934 - lr: 7.9433e-05\n",
      "Epoch 44/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4255 - Normal_Accuracy_Metric: 133.8373 - val_loss: 19.7362 - val_Normal_Accuracy_Metric: 175.0365 - lr: 7.0795e-05\n",
      "Epoch 45/1000\n",
      "746/746 [==============================] - 6s 9ms/step - loss: 11.4179 - Normal_Accuracy_Metric: 146.0974 - val_loss: 17.6590 - val_Normal_Accuracy_Metric: 173.7228 - lr: 6.3096e-05\n",
      "Epoch 46/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4306 - Normal_Accuracy_Metric: 145.7189 - val_loss: 14.8611 - val_Normal_Accuracy_Metric: 147.8111 - lr: 5.6234e-05\n",
      "Epoch 47/1000\n",
      "746/746 [==============================] - 7s 9ms/step - loss: 11.4304 - Normal_Accuracy_Metric: 121.7833 - val_loss: 13.9561 - val_Normal_Accuracy_Metric: 158.3804 - lr: 5.0119e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "history = DeepNet.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=BATCHSIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, reduce_learn_on_plateau,\n",
    "               cp_callback, learning_scheduler]  # Enter call back\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAm1klEQVR4nO3dfZyUdb3/8ddnbphVbuRG7kFBRVFBMVfSY5I3hUrmXR2F1NJMSsvUk6YdT+UpPfXLc+zU0SPH0rTyBvImLQk1NNHyhoVAIBAQQRYQdkHuWXZ35vP74zu7O7vMwt6ycM37+XgsM3Pdfudi5n195jvXXJe5OyIiEl2xjm6AiIi0LwW9iEjEKehFRCJOQS8iEnEKehGRiEt0dAPyOfjgg33IkCEd3QwRkf3GrFmzyt29d75x+2TQDxkyhJKSko5uhojIfsPMVjQ2Tl03IiIRp6AXEYk4Bb2ISMTtk330IlJ4qqqqKC0tpaKioqObsk8rKipi0KBBJJPJJs+joBeRfUJpaSldu3ZlyJAhmFlHN2ef5O6sX7+e0tJShg4d2uT59th1Y2aDzewVM1toZgvM7Ibs8J5m9pKZLcne9mhk/nPM7F0zW2pmtzW5ZSJSUCoqKujVq5dCfjfMjF69ejX7U09T+uirgW+5+9HAycDXzewY4DZgursPA6ZnHzdsVBy4DzgXOAaYkJ1XRGQXCvk9a8k22mPQu/sad5+dvb8FWAgMBC4AHslO9ghwYZ7ZRwNL3X2Zu1cCT2Tnaxc/n76EVxeXtdfiRUT2S8066sbMhgAnAG8Bfd19DYSdAdAnzywDgZU5j0uzw/Ite6KZlZhZSVlZy8L6/159j9cU9CLSQl26dOnoJrSLJge9mXUBngJudPfNTZ0tz7C8Vzpx9wfcvdjdi3v3zvsr3j1KJePsrM60aF4RkahqUtCbWZIQ8o+6+9PZwWvNrH92fH9gXZ5ZS4HBOY8HAatb3tzdK0rEqKhKt9fiRaRAuDu33HILI0aMYOTIkUyePBmANWvWMGbMGEaNGsWIESN47bXXSKfTXHnllbXT/vSnP+3g1u9qj4dXWuj5fxBY6O735Ix6DvgS8OPs7bN5Zp8JDDOzocAqYDzwhdY2ujFFyTgVquhF9nv//ocF/GN1UzsOmuaYAd34/mePbdK0Tz/9NHPmzGHu3LmUl5dz0kknMWbMGB577DHOPvtsbr/9dtLpNNu3b2fOnDmsWrWK+fPnA7Bx48Y2bXdbaEpFfypwBXCmmc3J/o0jBPynzWwJ8OnsY8xsgJlNBXD3auAbwAuEL3GnuPuCdngeAHRKxNipil5EWun1119nwoQJxONx+vbtyyc/+UlmzpzJSSedxK9+9SvuuOMO5s2bR9euXTnssMNYtmwZ119/PdOmTaNbt24d3fxd7LGid/fXyd/XDnBWnulXA+NyHk8Fpra0gc2hil4kGppaebcX97xfJTJmzBhmzJjB888/zxVXXMEtt9zCF7/4RebOncsLL7zAfffdx5QpU3jooYf2cot3L1Lnukmpj15E2sCYMWOYPHky6XSasrIyZsyYwejRo1mxYgV9+vThmmuu4eqrr2b27NmUl5eTyWT43Oc+xw9/+ENmz57d0c3fRaROgVCUjLNxR1VHN0NE9nMXXXQRb7zxBscffzxmxk9+8hP69evHI488wt13300ymaRLly78+te/ZtWqVVx11VVkMqE34Uc/+lEHt35X1thHlI5UXFzsLbnwyFd/U8KK9duZduOYdmiViLSnhQsXcvTRR3d0M/YL+baVmc1y9+J800es60bH0YuINBSpoC9Kqo9eRKShiAV9XEEvItJApII+lYip60ZEpIFIBX1NRb8vfsEsItJRIhX0qUSMjENVWkEvIlIjUkFflIwDsLNa/fQiIjUiFfSpbNBXVKmfXkTa1+7OXb98+XJGjBixF1uze9EK+kR4OqroRUTqRO4UCKCKXmS/96fb4MN5bbvMfiPh3B83OvrWW2/l0EMP5brrrgPgjjvuwMyYMWMGH330EVVVVdx5551ccEHzroZaUVHBtddeS0lJCYlEgnvuuYczzjiDBQsWcNVVV1FZWUkmk+Gpp55iwIABXHLJJZSWlpJOp/nud7/LpZde2qqnDVEL+mxFr2PpRaS5xo8fz4033lgb9FOmTGHatGncdNNNdOvWjfLyck4++WTOP//8Zl2g+7777gNg3rx5LFq0iLFjx7J48WImTZrEDTfcwGWXXUZlZSXpdJqpU6cyYMAAnn/+eQA2bdrUJs8tUkGfqv0yVhW9yH5tN5V3eznhhBNYt24dq1evpqysjB49etC/f39uuukmZsyYQSwWY9WqVaxdu5Z+/fo1ebmvv/46119/PQDDhw/n0EMPZfHixZxyyincddddlJaWcvHFFzNs2DBGjhzJzTffzK233sp5553Haaed1ibPLVJ99DUVvS4+IiIt8fnPf54nn3ySyZMnM378eB599FHKysqYNWsWc+bMoW/fvlRUVDRrmY39rucLX/gCzz33HAcccABnn302L7/8MkceeSSzZs1i5MiRfOc73+EHP/hBWzytaFb0FfoyVkRaYPz48VxzzTWUl5fz6quvMmXKFPr06UMymeSVV15hxYoVzV7mmDFjePTRRznzzDNZvHgxH3zwAUcddRTLli3jsMMO45vf/CbLli3jnXfeYfjw4fTs2ZPLL7+cLl268PDDD7fJ84pU0Bclayp6dd2ISPMde+yxbNmyhYEDB9K/f38uu+wyPvvZz1JcXMyoUaMYPnx4s5d53XXX8bWvfY2RI0eSSCR4+OGHSaVSTJ48md/+9rckk0n69evH9773PWbOnMktt9xCLBYjmUxy//33t8nzitT56JeXb+P0//wLP730eC46YVA7tExE2ovOR990zT0f/R4rejN7CDgPWOfuI7LDJgNHZSfpDmx091F55l0ObAHSQHVjjWgrqWTNUTeq6EVEajSl6+Zh4F7g1zUD3L32wE4z+y9gd8cAneHu5S1tYHMUJbJH3ejLWBHZC+bNm8cVV1xRb1gqleKtt97qoBblt8egd/cZZjYk3zgLB5NeApzZxu1qkdofTOnwSpH9krs36xj1jjZy5EjmzJmzV9fZku721h5eeRqw1t2XNDLegRfNbJaZTdzdgsxsopmVmFlJWVlZixpTewoEdd2I7HeKiopYv369TjO+G+7O+vXrKSoqatZ8rT3qZgLw+G7Gn+ruq82sD/CSmS1y9xn5JnT3B4AHIHwZ25LGxGJGp3hMh1eK7IcGDRpEaWkpLS30CkVRURGDBjXvYJMWB72ZJYCLgRMbm8bdV2dv15nZM8BoIG/Qt5VUQteNFdkfJZNJhg4d2tHNiKTWdN18Cljk7qX5RppZZzPrWnMfGAvMb8X6miSVjOsUCCIiOfYY9Gb2OPAGcJSZlZrZ1dlR42nQbWNmA8xsavZhX+B1M5sLvA087+7T2q7p+RUlVdGLiORqylE3ExoZfmWeYauBcdn7y4DjW9m+ZkslYvoyVkQkR6ROagbhEEtdeEREpE4kg16/jBURqRO5oE8lYqroRURyRC7oVdGLiNQXuaDXcfQiIvVFLuiLdBy9iEg9EQx6VfQiIrkiF/SpRFxBLyKSI3pBn4yp60ZEJEfkgr4oEfrodapTEZEgckFfczlBVfUiIkHkgr7ucoIKehERiGDQ114gXL+OFREBIhj0quhFROqLXtDXXiBcFb2ICEQw6GsuEK5j6UVEgsgFfU1Fr6NuRESCCAa9KnoRkVyRC/pU9stYnapYRCRoysXBHzKzdWY2P2fYHWa2yszmZP/GNTLvOWb2rpktNbPb2rLhjSmq/cGUKnoREWhaRf8wcE6e4T9191HZv6kNR5pZHLgPOBc4BphgZse0prFNUXvUjSp6ERGgCUHv7jOADS1Y9mhgqbsvc/dK4AngghYsp1l01I2ISH2t6aP/hpm9k+3a6ZFn/EBgZc7j0uywdpXSUTciIvW0NOjvBw4HRgFrgP/KM43lGdboKSXNbKKZlZhZSVlZWQubpYpeRKShFgW9u69197S7Z4BfELppGioFBuc8HgSs3s0yH3D3Yncv7t27d0uaBYSgN1NFLyJSo0VBb2b9cx5eBMzPM9lMYJiZDTWzTsB44LmWrK+ZbSOViLFTFb2ICACJPU1gZo8DpwMHm1kp8H3gdDMbReiKWQ58NTvtAOCX7j7O3avN7BvAC0AceMjdF7THk2hIlxMUEamzx6B39wl5Bj/YyLSrgXE5j6cCuxx62d6KdDlBEZFakftlLIRj6VXRi4gEkQz6VCKmH0yJiGRFMuiLknGdAkFEJCuSQa+KXkSkTiSDXhW9iEidSAZ9OLxSFb2ICEQ16JMxXTNWRCQrkkFflIizUxW9iAgQ1aBPxtRHLyKSFcmgVx+9iEidSAa9KnoRkTqRDPpUIk5V2klnGj39vYhIwYhk0NdcIFznuxERiWzQ63KCIiI1Ihn0upygiEidSAa9KnoRkToRDXpV9CIiNSIZ9KlEqOgV9CIiUQ36bEWvrhsRkagGvSp6EZFaewx6M3vIzNaZ2fycYXeb2SIze8fMnjGz7o3Mu9zM5pnZHDMracN271ZdH70qehGRplT0DwPnNBj2EjDC3Y8DFgPf2c38Z7j7KHcvblkTm6/uqBtV9CIiewx6d58BbGgw7EV3r84+fBMY1A5ta7Ga4+h1qmIRkbbpo/8y8KdGxjnwopnNMrOJu1uImU00sxIzKykrK2tVg1TRi4jUaVXQm9ntQDXwaCOTnOruHwPOBb5uZmMaW5a7P+Duxe5e3Lt379Y0qzbo1UcvItKKoDezLwHnAZe5e97TRLr76uztOuAZYHRL19ccOgWCiEidFgW9mZ0D3Aqc7+7bG5mms5l1rbkPjAXm55u2rSXjMeIx03H0IiI07fDKx4E3gKPMrNTMrgbuBboCL2UPnZyUnXaAmU3NztoXeN3M5gJvA8+7+7R2eRZ5pBIxVfQiIkBiTxO4+4Q8gx9sZNrVwLjs/WXA8a1qXSsUJeNU6MtYEZFo/jIWoCgR0+GVIiJEOOhTyTgV6qMXEYlw0KuPXkQEiHDQFyXjOupGRIQIB70qehGRILJBr4peRCSIbNCnEjF2qqIXEYlu0Bcl4+q6EREh0kEfU9eNiAgRDvpUQhW9iAhEOOiLkjGdplhEhEgHfZyd1WkaOYOyiEjBiGzQpxIxMg5VaQW9iBS2yAZ97VWmdAZLESlwkQ16XSBcRCSIbtDXXjdWFb2IFLbIBn1N142OpReRQhfZoNcFwkVEgsgGfV1Fr6AXkcLWlIuDP2Rm68xsfs6wnmb2kpktyd72aGTec8zsXTNbama3tWXD96RIX8aKiABNq+gfBs5pMOw2YLq7DwOmZx/XY2Zx4D7gXOAYYIKZHdOq1jZDSodXiogATQh6d58BbGgw+ALgkez9R4AL88w6Gljq7svcvRJ4IjvfXlGUrOmjV0UvIoWtpX30fd19DUD2tk+eaQYCK3Mel2aH5WVmE82sxMxKysrKWtisOkUJ9dGLiED7fhlreYY1ej4Cd3/A3Yvdvbh3796tXnlKFb2ICNDyoF9rZv0Bsrfr8kxTCgzOeTwIWN3C9TVbbUWvwytFpMC1NOifA76Uvf8l4Nk808wEhpnZUDPrBIzPzrdX1Fb0+sGUiBS4phxe+TjwBnCUmZWa2dXAj4FPm9kS4NPZx5jZADObCuDu1cA3gBeAhcAUd1/QPk9jVzUVvX4wJSKFLrGnCdx9QiOjzsoz7WpgXM7jqcDUFreuFWIxo1NclxMUEYnsL2MhnAZBFb2IFLpoB30yrqNuRKTgRTroi5IxHUcvIgUv0kGfSsR0rhsRKXiRDvqiZFx99CJS8CId9KmEjroREYl00KuiFxEpgKBXRS8ihS7SQa/j6EVEIh70Rcm4LjwiIgUv4kGvwytFRCId9KmEvowVEYl20CdjOk2xiBS8aAd9Ik5ldQb3Ri9sJSISeZEO+poLhOsQSxEpZNEO+trLCSroRaRwRTro6y4nqC9kRaRwRTrodTlBEZGoB30y23WjPnoRKWAtDnozO8rM5uT8bTazGxtMc7qZbcqZ5nutbnEzpBLZrhtV9CJSwPZ4cfDGuPu7wCgAM4sDq4Bn8kz6mruf19L1tEZNRa/LCYpIIWurrpuzgPfcfUUbLa9NpGoPr1RFLyKFq62CfjzweCPjTjGzuWb2JzM7trEFmNlEMysxs5KysrI2aVTdl7Gq6EWkcLU66M2sE3A+8Ls8o2cDh7r78cD/AL9vbDnu/oC7F7t7ce/evVvbLKDuB1PqoxeRQtYWFf25wGx3X9twhLtvdvet2ftTgaSZHdwG62ySVEJH3YiItEXQT6CRbhsz62dmlr0/Oru+9W2wziZRRS8i0oqjbgDM7EDg08BXc4Z9DcDdJwGfB641s2pgBzDe9+IZxlI6jl5EpHVB7+7bgV4Nhk3KuX8vcG9r1tEaOo5eRCTiv4ytCfqdCnoRKWCRDnozI5WIqetGRApapIMeshcIV0UvIgWsAII+ph9MiUhBi3zQpxJxnQJBRApa5INeFb2IFLoCCPq4rjAlIgUt8kGfSsR0zVgRKWiRD3pV9CJS6CIf9KroRaTQRT/oVdGLSIGLfNAXJeKq6EWkoEU+6FPJmI6jF5GCFvmgL0rEdRy9iBS06Ad9MqZz3YhIQYt80KcScaozTnVaVb2IFKbIB33N5QR1qmIRKVSRD/rai48o6EWkQEU+6Iuy141VP72IFKpWBb2ZLTezeWY2x8xK8ow3M/u5mS01s3fM7GOtWV9LKOhFpNC16uLgWWe4e3kj484FhmX/Pg7cn73da9R1IyKFrr27bi4Afu3Bm0B3M+vfzuusRxW9iBS61ga9Ay+a2Swzm5hn/EBgZc7j0uywXZjZRDMrMbOSsrKyVjarTip71I1+NCUihaq1QX+qu3+M0EXzdTMb02C85ZnH8y3I3R9w92J3L+7du3crm1UnlQgVvU6DICKFqlVB7+6rs7frgGeA0Q0mKQUG5zweBKxuzTqbq0gVvYgUuBYHvZl1NrOuNfeBscD8BpM9B3wxe/TNycAmd1/T4ta2gCp6ESl0rTnqpi/wjJnVLOcxd59mZl8DcPdJwFRgHLAU2A5c1brmNl/tL2NV0YtIgWpx0Lv7MuD4PMMn5dx34OstXUdbqD3qRhW9iBSoyP8ytvY4elX0IlKgIh/0Oo5eRApd5IM+GY8Rj5m6bkSkYEU+6CF036jrRkQKVUEEfVEyropeRApWYQR9IqYfTIlIwSqIoE8l4zp7pYgUrMII+oQuEC4ihaswgl4VvYgUsIII+iJV9CJSwAoj6JNxdiroRaRAFUTQpxIxdd2ISMEqiKAvSsbVdSPRsHkNrH+vo1sh+5kCCXodRy8RsH0DPDgW7v8nWPiHjm6N7EcKIuhTibguPCL7N3d49uuwZQ0cPAwmXwFv/6KjWyX7idZceGS/oYq+DbjD1nVQtqjub90i2LEBTrwKiq+CRGrvt6vsXVjwezjxSujat/3WU70Tnp4I29eH5zr8s5Do1H7ra+jN/4V3p8LZPwrP9amvwNSbYVMpnPV9iO3jNdvOrfDhO3DIKWD5LiXd2HxboLoybOt4CuLJ5s0vQIEEfSoRznXj7lhbvEiqK2HnZuh8cOuXta/bvgH+cAMsfw12fFQ3PHUQ9BkOqW4w7VZ44z44/TY47lKI74WXVflSePX/wbzfAQ6zfgWX/AYGn9T260pXwZNfhkV/hG4Dw/0ufUPgnngldBvQ9uvMVToLXvo+HPUZOPnaEHSX/gam3gJ//W/YvBouuG/v7niayh3mPwUvfhe2rIZRl8F5P91zUeAOb02CF/8NMtU5IyzMG0/BwUfA5x6EnkPb9SnUU10JpW/Dey/D0unh+5LeR0H/46DfSOh3HPQ5BjoduPfa1AQWLgK1bykuLvaSkpI2W969Ly/hP19czOI7z6VTohWVz9Z1UPIrKHkQtq6Fo8bBP13f/CqlNXZuhQ3vQfkSqNoBx14EqS7ts64N78Oj/wwbV8Dx46HPseFF3efoEHRm4Q257BWY/gNY/Xc4+Eg489/g6PObt03S1SGsl/0F+o+CQ06GgSfu+obZsAxevRveeQISRTD6Gjji03XdGuPuDuHbmEwG5j8Jr90DR5wVquHdBWQmHSr5+U/CuXfDSV+BpX+Gmb+EJS+CxWD4Z8LwIac1rbLeuBJm/gLmPBaC4fyfw0GD8k+74yOYNCbc/9oMOKBH3Th3eP2esO2HfhIu/S0Uddvz+veWD+fDn74NK/4K/Y8P75O3JsHgj4e2dumTf77K7aG4mDcFjjwHDj8TqitCyKZ3hk9X1TvD+FgCvjAFBn6s/Z7HhvfD//nS6aHgqdwKFofBo8N7oWwxfDgPdm4K01sMeg0Lr+Ejzw7/N+31Hs1hZrPcvTjvuEgF/d/uhb7HhjdcTlX5y9eWcefzC3nnjrF0K0o2f7lr5sKbk8KbPV0JR3wqrGf2b0LXxcDiEPhHfxZi8eYvvzE7t4SKdc07sH5p+NvS4NrqBx4Mn7gRiq9u2yqitAQeuzRUU+MfgyGn7n569/AF4ct3Qvm7Iaw/eSsMG7vnCv+9l2Had0J3UNcBofKD8CbufzwMPhkGFYfp5jwWPr6f9BU49Ya6sNi+AZ66Okxz4pVw7k92rRqXvQovfTf8f3Y/BDZ+EHYmn38IegzZtV2ZDPzhm/D338Cn7oBP3FR//Ib3oeShMH7HR9C5Dxw5Fo48Fw4/Azp1rr99Vr4Nb90P/3gOcDj8LFjxt/A8x90Nx11Sf+foDpMvh8XT4MsvhG2Qz5zH4blvQO/hcOH9obpsrS0fwoJnwjIPOQWSRU2fd8dGeOU/ws6sqDuc9T342BfDe2PBM/DMteHT8ITHQxWc66Pl4Tl/OB/OuB1O+1bjO8/yJfDbi2FbOfzzwyFU20q6OnSVzfwlvP9qGNZjSPg/O/xMGDqm/k7VPRREH84Lf2vmwvK/QuUWiHeCIZ+AYWeH10fPw9qunTnaJejNbDDwa6AfkAEecPefNZjmdOBZ4P3soKfd/Qd7WnaLgr5yG/zX0WGvemCvUFEeexEceiq/ebuU7z67gJm3f4reXffwkbFyW6jct5WF6nHWw/DBG5DsDKMmwOivQu8js9NuhzmPhm6Lj94PL4STvx7+Mw/sBZ26tKzS31YeKp+3fwEVG+GAntDriOzf4eHLuF5HhB3BX34cKuoufeET/xJCrjlvynwW/gGeuiaE6OVPhfU1VSYN70yGV34Emz4I4XfcJTDqC2HnmGv9e/DC7bD4T2Hbjb0rVMcVG2HlzLDdV74Fq2aFii6eguIvhx1b13751/3ynaHKHXRS6Mrp1h/W/gNe+h4sfQkOGhyCZ8TnYdEf4Nnrw7wX3hd21DXc4U+3wtv/B2O+DWfe3vhzrtoBi54PwbDkz+E1GE/BYZ8MFWmnzvDW/8Hq2aHL68QvwuiJYWezYVkIvpVvhtfseT+t6xJ8836YdlvYLv/0jd1v96XT4XdXhi7FQSeFHf+xFzX/tVC5LRRMf/0ZVG0LwxIHhB394WeGv97D617X7lCxKXQfbV4NZQvh9f8OBVDxl0NYH9iz/jpWz4HHJ4T/54sfqNvuS6eHbjEcLv5leB/tyZa18Nglof//M/eE709aY+s6mPVI+HS5eRV0GxSWOeLi5gd0dWV4DS95ERa/AOuXhOE9Dw/v324Dwl/X/uF12m1guH9A9xY1vb2Cvj/Q391nm1lXYBZwobv/I2ea04Gb3f285iy7xRV91Y7wEWvBM/DutPBCPfBglvY+kx8vGcxNZw6ld2IbnSo3kdy5kUTlR8R3biS+fT1sK8O2rqt7cdfofkgI9xMub/w/IJMOb/S//RxKZ9YNj3cKgX9gr/Bi79w7fNTrPyp8ZG/45eFHK+CNe8MnheodMPy8UEU2VsnVWPE3ePkuWPF6qIjHfAuGjIHNpaGbYFNp9m9leCH3Py5bmZyxa2C+8b/wwr+GSnfCE9Cld1O2/K7SVbDkpbAjXDwtfDLof3zoox02NnR/vTkpVN1jboaTr2u837a6EtbOy74R8gR8Qwt+D7+/LgTsYaeHT2KprnDazSFgc8Nvw/shXFbPDv/PY38Y/t/+fEfo/z7lGzD2zqbvsNNV4f9j8bQQ/B8tD8N7HQEf/xocP2HXj/GZNPztf+CVu0IFfP7Pw072wbPDp8cJjzdt/ds3wNwnwqeM9UtCN8+oy0Lg9jp89/Nm0uH/6uW7YOuHYafzyVtDeL83PXxSKl8cpu3aPzyfLR+G8Q3fM4NPDp9QdvfJYsuH8MRlsKoEzvi3UO1P/0Ho3770N3tub66dW+HJq0KgnnZz6DrMt72qKsJ7oHJruF+1PRQQVTvC3/uvhtdOpiq8bk66Juyo2+o7pw3LYPGL8P6MUARtXh2+3M91QA+4dXmLFr9Xum7M7FngXnd/KWfY6ezNoM9Vub029NOL/kQ8vaPe6B3eiY10YaN3ZoN3o5yD2MBBbLTufBTrwaZYdzbGe7I8MYRYPEkiZsRjVntpwkTMSMTD43A/RsLg8KrFDKhaQdfMJrqkN9M1s5HO1ZvonN5E16pyelTWdb1sTR7Mus5Hsa7LUXTfuYYjy17EzVjabxzzhlzJ1i6HEY8Z8ViMeAxiZtnHVnvfwnbGcHqVvckR839O9/Wz6z1Xtxg7D+hLZecBVKd60KX873SqCC+w7T2OYsvAMWwZeBoHrXyF3v/4FRsPPZuVZ/wMTxxAWEN435iBYcRi2duaYRbaY4Q2Ws5wA+I7yil69/cc+I/JJNfNC23CqBgxge2nfQc6962bj7o3qLPra7N2fG176tZT006AWNkiOj15Bba5lHTxV6g+9VtwQI/a8YbVzZ+pIvbn72Nv3R92Rod+At68LwTkZ+5p+fcv7uGooB0bQvjtqf/+w/nwzFdh7fzwJXfRQfDVGbtWxE1Z7/szQuAv+mPYyQ4+ORQZPYeGyrTH0HC/U+fwPnnxe7BuQfg0MPYuOOTjuy5348rw6XHp9BBSNVVot4HZ6jR7e9Cgpm2zqorQNfbO5PD42Ivhgnvrd3k1Vboanv8XmP0IHDcePj4x9J2XLQo7qLJFYafruzn6LtUtfPI86SvN+xTbGtU7Q3fs5jXhE0T1TjjhshYtqt2D3syGADOAEe6+OWf46cBTQCmwmhD6CxpZxkRgIsAhhxxy4ooVK1rdrhrVFVtZMOt1NnsR22Ld2BbrynY6UVWdoTKdobI6Q3U6Q2XaqU5nqEpnqMo4VdUZqjNOdcZJZzJUpZ109nF1OkN12qnKhNvaYRmnOpMhk4HqTIZ0BjIexmUcDvRtHJl5n2PsfY5mOUfbcoZZKRV04rH0WTxYfS4f0qsVz9Y5JfYP+vIRq/xgVnsv1tKD6pwDrIwMR9sHnBabx5jYOxTH3iVl4ciGX1afy39UX0amnX5icbSt4IzYHGZkRjLf26evskYRO+lCBeUc1KTpPxWbxX8mJ9HdtvFU+jS+Xf1VnFi9I7Ua7lAMI2ff06jcnZjlTN/wKLAkVVzH77iYV/gmtzCXI8n3Ho1ld/YxCzvX8DiMc6d2F9kz8xEX+nROy8xkEGvpzpZ6y9lIV7qzhVL68D+xy5luDQ8syFlHzc7b6r6Hz21aw3Y26Qg3dy5Mv4BjPBsfi8XqdvhWt2lx6taVrwCoWdYXq5/kmqrHagdVkaDU+rM8NpgVNoiVsf5s40B2Wid2ksredqKCFB/ZQVRZUf0Cwnb//9pQw+dcs0289p/d69G5E09d+0/NWGO9dbdf0JtZF+BV4C53f7rBuG5Axt23mtk44GfuvsddZVsfdbOv88rtZNxJxw8gnXHSHnYoNX8ZDzuSTM0wz953xz3sSGrfBDWPCS+yjIdbBzKZ7GMazFe1jYPWvo1bjA39T6u3LKB2WbW3Tu1yapefs7ya25r11CyjZpk1w3OXV9tW6r+xGn43mdue3OU1HJ87T8Px5K43Z5qMQ9eKNRy68U3m9zkPt0T9efKsZ4/hk52w7vnntns3s3j95567TZz62zqd/X/NZLxuJ2J1c+XuWIrSW+i5cxU9K1fRa+cqelau5sOiw3m71wWkY7sefVT7OsoWLLmvn5pV1PvUlzNf7nOp2Ua2m9j07Iaq/3rLv2PcXfgO3T6XztWbWJs6lPJOA8lY/a6XxuZtuN7c12eT0r7Ba8UaVAD5duwNdS1K8B8XjdztNI1pt6A3syTwR+AFd7+nCdMvB4rdvXx30xVa0IuItNbugr7Fn88t7JoeBBY2FvJm1i87HWY2Oru+9fmmFRGR9tGar5NPBa4A5pnZnOywfwUOAXD3ScDngWvNrBrYAYz3ffHAfRGRCGtx0Lv76+yh58rd7wXubek6RESk9fbxMyGJiEhrKehFRCJOQS8iEnEKehGRiFPQi4hE3D55mmIzKwNaeg6Eg4Hd/iCrQGg7BNoOgbZDEOXtcKi75z0L4T4Z9K1hZiWN/TqskGg7BNoOgbZDUKjbQV03IiIRp6AXEYm4KAb9Ax3dgH2EtkOg7RBoOwQFuR0i10cvIiL1RbGiFxGRHAp6EZGIi0zQm9k5ZvaumS01s9s6uj17k5k9ZGbrzGx+zrCeZvaSmS3J3vboyDbuDWY22MxeMbOFZrbAzG7IDi+obWFmRWb2tpnNzW6Hf88OL6jtUMPM4mb2dzP7Y/ZxwW2HSAS9mcWB+4BzgWOACWZ2TMe2aq96GDinwbDbgOnZSzdOzz6OumrgW+5+NHAy8PXs66DQtsVO4Ex3Px4YBZxjZidTeNuhxg3AwpzHBbcdIhH0wGhgqbsvc/dK4Anggg5u017j7jOADQ0GXwA8kr3/CHDh3mxTR3D3Ne4+O3t/C+HNPZAC2xYebM0+TGb/nALbDgBmNgj4DPDLnMEFtx2iEvQDgZU5j0uzwwpZX3dfAyEAgT4d3J69ysyGACcAb1GA2yLbXTEHWAe85O4FuR2A/wa+DWRyhhXcdohK0Oe70pWOGy1QZtYFeAq40d03d3R7OoK7p919FDAIGG1mIzq4SXudmZ0HrHP3WR3dlo4WlaAvBQbnPB4ErO6gtuwr1ppZf4Ds7boObs9eYWZJQsg/6u5PZwcX5LYAcPeNwF8I3+EU2nY4FTjfzJYTunPPNLPfUnjbITJBPxMYZmZDzawTMB54roPb1NGeA76Uvf8l4NkObMteYWYGPAgsdPd7ckYV1LYws95m1j17/wDgU8AiCmw7uPt33H2Quw8hZMLL7n45BbYdIEK/jDWzcYT+uDjwkLvf1bEt2nvM7HHgdMIpWNcC3wd+D0wBDgE+AP7Z3Rt+YRspZvYJ4DVgHnV9sv9K6KcvmG1hZscRvmSME4q5Ke7+AzPrRQFth1xmdjpws7ufV4jbITJBLyIi+UWl60ZERBqhoBcRiTgFvYhIxCnoRUQiTkEvIhJxCnoRkYhT0IuIRNz/B3VIw/YHPhLWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss and validation curves vs epoch\n",
    "history_df = pd.DataFrame(history.history)\n",
    "np.log(history_df.loc[:, [\"loss\", \"val_loss\"]]).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss: 11.4197416305542\n"
     ]
    }
   ],
   "source": [
    "# Output to the console the minimum epoch\n",
    "print(\"Minimum validation loss: {}\".format(history_df[\"val_loss\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65c5a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 304ms/step\n",
      "[[ 2.5968558e+02  5.7332975e-01  1.3555371e+00  1.1466875e+01\n",
      "  -1.2017314e+00 -1.0263433e+00  2.6716202e-01 -2.2726102e-01\n",
      "   3.3133459e-01]]\n",
      "[1.37346188e+05 8.16028237e-01 1.20712149e+00]\n",
      "1/1 [==============================] - 0s 304ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=12.497163>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-0.87595844>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-0.9405606>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the momentas for a single jet and determine the loss\n",
    "print(DeepNet.predict(tracks[:1]))\n",
    "print(bhads[0])\n",
    "\n",
    "# Calculate the individual loss for each feature\n",
    "LogNormal_Loss_Function_Check(bhads[0],DeepNet.predict(tracks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 - 3s - loss: 11.4325 - Normal_Accuracy_Metric: 106.1589 - 3s/epoch - 1ms/step\n",
      "The Loaded DeepNet has loss:  [11.432482719421387, 106.15887451171875] 3\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the entire performance of the model\n",
    "loss = DeepNet.evaluate(tracks,bhads,verbose = 2)\n",
    "print(\"The Loaded DeepNet has loss: \", loss,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a05ddcd8ffea9a6a7d2e914b733df5445b717626b5b8c92c04bfc4eb6e7f5cba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
