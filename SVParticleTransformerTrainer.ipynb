{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 16:14:32.587709: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 16:14:32.765378: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-15 16:14:32.769641: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-15 16:14:32.769661: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-15 16:14:33.827672: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 16:14:33.827744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 16:14:33.827751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of jets to train on:\n",
      "141329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/physics/phujdj/DeepLearningParticlePhysics/hffrag.py:136: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = numpy.where(mask, pts, pts * numpy.sinh(etas))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "#from wandb.keras import WandbCallback\n",
    "from keras import callbacks\n",
    "import keras\n",
    "import DeepSetNeuralNetArchitecture as DSNNA\n",
    "from DeepSetNeuralNetArchitecture import LogNormal_Loss_Function\n",
    "from DeepSetNeuralNetArchitecture import Mean_Squared_Error\n",
    "from HffragDeepSetsProjectionMultivariate import DeepSetsProjection\n",
    "import hffragTransformerJetMultivariateSVFinder as hffragT\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import keras.backend as k\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import sklearn as sk\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from hffrag import fixedbinning\n",
    "from hffrag import binneddensity\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['savefig.facecolor'] = 'white'\n",
    "plt.rc('text',usetex = False)\n",
    "plt.rc('font',family = 'Times New Roman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is being stored in a tree datastructure.\n",
    "# We access the charm root using this command\n",
    "tree = uproot.open(\"/storage/epp2/phswmv/data/hffrag/hffrag.root:CharmAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "MASKVAL = -999 # This value is introduced to ensure arrays are regular (Of the same size). They will be masked later by the network\n",
    "MAXTRACKS = 40 # This value is the maximum number of tracks allowed per event\n",
    "BATCHSIZE = 64 # This is the batch size of the mini batches used during training\n",
    "EPOCHS = 1000  # This is the default number of epochs for which the neural network will train providing that early stopping does not occur\n",
    "MAXEVENTS = 5e5 #This is the maximum number of events that will the program will accept\n",
    "LR = 1e-4 #This is the default learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we wish to study\n",
    "track_features = [\"AnalysisTracks_pt\", \"AnalysisTracks_eta\", \"AnalysisTracks_phi\", \"AnalysisTracks_z0sinTheta\",\n",
    "                  \"AnalysisTracks_d0sig\", \"AnalysisTracks_d0\", \"AnalysisTracks_d0sigPV\", \"AnalysisTracks_d0PV\"]\n",
    "jet_features = [\"AnalysisAntiKt4TruthJets_pt\", \"AnalysisAntiKt4TruthJets_eta\", \"AnalysisAntiKt4TruthJets_phi\", \"AnalysisAntiKt4TruthJets_m\",\"AnalysisAntiKt4TruthJets_ghostB_pdgId\",\n",
    "                \"AnalysisAntiKt4TruthJets_ghostB_pt\", \"AnalysisAntiKt4TruthJets_ghostB_eta\",\"AnalysisAntiKt4TruthJets_ghostB_phi\", \"AnalysisAntiKt4TruthJets_ghostB_m\"]\n",
    "SV_features = [\"TruthParticles_Selected_LxyT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the root file\n",
    "features = tree.arrays(jet_features+track_features + SV_features, entry_stop=MAXEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the events of interest\n",
    "events = features[ak.sum(\n",
    "    features[\"AnalysisAntiKt4TruthJets_pt\"] > 25000, axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jets to train on is:  299742\n",
      "The number of track features is:  8\n"
     ]
    }
   ],
   "source": [
    "# Displays the number of jets being trained on\n",
    "jets = events[jet_features][:,0]\n",
    "print(\"The number of jets to train on is: \", len(jets))\n",
    "print(\"The number of track features is: \",len(track_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tracks from the events\n",
    "tracks = events[track_features]\n",
    "Secondary_Displacement_temp_first = events[SV_features]\n",
    "\n",
    "# Match the tracks to the jets\n",
    "mask = DSNNA.Match_Tracks(jets, tracks)\n",
    "matchedtracks = tracks[mask]\n",
    "\n",
    "# Pad and Flatten the data\n",
    "matchedtracks = DSNNA.flatten(matchedtracks, MAXTRACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 outputs\n",
      "There are 32 inputs\n"
     ]
    }
   ],
   "source": [
    "bjets = ak.sum(jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] > 5000, axis=1) > 0\n",
    "Secondary_Displacement_temp_a = DSNNA.flatten(Secondary_Displacement_temp_first, 6).to_numpy()\n",
    "Secondary_Displacement_temp_b = Secondary_Displacement_temp_a[:,0]\n",
    "Secondary_Displacement_temp = Secondary_Displacement_temp_b[bjets]\n",
    "jets = jets[bjets]\n",
    "\n",
    "# Obtain the pt, eta and phi of each b hadron jet\n",
    "bhads_pt = jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"][:, 0].to_numpy()\n",
    "bhads_eta = jets[\"AnalysisAntiKt4TruthJets_ghostB_eta\"][:,0].to_numpy()\n",
    "bhads_phi = jets[\"AnalysisAntiKt4TruthJets_ghostB_phi\"][:,0].to_numpy()\n",
    "bhads_m = jets[\"AnalysisAntiKt4TruthJets_ghostB_m\"][:,0].to_numpy()\n",
    "bhads_PGID = jets[\"AnalysisAntiKt4TruthJets_ghostB_pdgId\"][:,0].to_numpy()\n",
    "\n",
    "jets_pt = jets[\"AnalysisAntiKt4TruthJets_pt\"].to_numpy()\n",
    "jets_eta = jets[\"AnalysisAntiKt4TruthJets_eta\"].to_numpy()\n",
    "jets_phi = jets[\"AnalysisAntiKt4TruthJets_phi\"].to_numpy()\n",
    "jets_m = jets[\"AnalysisAntiKt4TruthJets_m\"].to_numpy()\n",
    "b_jets = np.stack([jets_pt,jets_eta,jets_phi, jets_m], axis = -1)\n",
    "\n",
    "bhads = np.stack([bhads_pt,bhads_eta,bhads_phi, bhads_m],axis = -1) #Combine the momentum, eta and phi for each jet into one array\n",
    "\n",
    "print(\"There are {} outputs\".format(np.shape(bhads)[1])) # Display the number of target features the neural network will predict\n",
    "matchedtracks = matchedtracks[bjets]\n",
    "print(\"There are {} inputs\".format(np.shape(matchedtracks)[1])) # Display the number of target features the neural network will use in it's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-999.0 115.98360443115234\n"
     ]
    }
   ],
   "source": [
    "Secondary_Displacement = np.array([x[0] for x in Secondary_Displacement_temp])\n",
    "print(np.min(Secondary_Displacement), np.max(Secondary_Displacement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the jet and tracks to unstructed data.\n",
    "jets = structured_to_unstructured(jets[jet_features[:-5]])\n",
    "matchedtracks = structured_to_unstructured(matchedtracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144756, 32, 3)\n",
      "(144756, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/physics/phujdj/DeepLearningParticlePhysics/DeepSetNeuralNetArchitecture.py:104: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = np.where(mask1 | mask3, pts, pts * np.sinh(etas))\n"
     ]
    }
   ],
   "source": [
    "# Convert the coordinates of the b jets and tracks to cartesian coordinates\n",
    "polartracks = matchedtracks.to_numpy()\n",
    "Num_events = MAXTRACKS\n",
    "tracks_pt = polartracks[:,:,0].reshape(-1,Num_events,1)\n",
    "tracks_eta = polartracks[:,:,1].reshape(-1,Num_events,1)\n",
    "tracks_phi = polartracks[:,:,2].reshape(-1,Num_events,1)\n",
    "\n",
    "tracks_pep = np.concatenate([tracks_pt,tracks_eta,tracks_phi], axis = -1) \n",
    "print(tracks_pep.shape)\n",
    "\n",
    "jets_pt = b_jets[:,0].reshape(-1,1)\n",
    "jets_eta = b_jets[:,1].reshape(-1,1)\n",
    "jets_phi = b_jets[:,2].reshape(-1,1)\n",
    "\n",
    "b_jets_pep = np.concatenate([jets_pt,jets_eta,jets_phi], axis = -1) \n",
    "print(b_jets_pep.shape)\n",
    "\n",
    "tracks_p = DSNNA.pt_eta_phi_2_px_py_pz_tracks(matchedtracks.to_numpy())\n",
    "bhads = DSNNA.pt_eta_phi_2_px_py_pz_jets(bhads)\n",
    "b_jets_p = DSNNA.pt_eta_phi_2_px_py_pz_jets(b_jets)\n",
    "b_jets_m = b_jets[:,-1].reshape(-1,1)\n",
    "\n",
    "#Combine the momenta of the tracks with the rest of the track features to form the track dataset\n",
    "tracks = np.concatenate([tracks_p,tracks_pep,matchedtracks[:,:,3:].to_numpy()],axis = 2)\n",
    "b_jets = np.concatenate([b_jets_p,b_jets_pep,b_jets_m] ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.ma.masked_values(tracks,-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144756,)\n"
     ]
    }
   ],
   "source": [
    "bhads_fractions_px = bhads[:,0]/b_jets[:,0]\n",
    "bhads_fractions_py = bhads[:,1]/b_jets[:,1]\n",
    "bhads_fractions_pz = bhads[:,2]/b_jets[:,2]\n",
    "print(bhads_fractions_px.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144756, 3)\n"
     ]
    }
   ],
   "source": [
    "b_jets_mag = np.linalg.norm(b_jets[:,:3], axis = 1)\n",
    "bhads_fractions = np.stack([bhads_fractions_px,bhads_fractions_py, bhads_fractions_pz], axis = -1)\n",
    "bhads_projection = ((bhads[:,:3]*b_jets[:,:3]).sum(axis = 1))/(b_jets_mag**2)\n",
    "print(bhads_fractions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1431.3222789813972 -4195.466731248074\n",
      "(111247, 6)\n",
      "1.4998932008025052 0.00039231462406675546\n",
      "(111247,)\n"
     ]
    }
   ],
   "source": [
    "print(np.max(bhads_fractions), np.min(bhads_fractions_px))\n",
    "array = [x for x in range(bhads_fractions_px.shape[0])]\n",
    "bhads_trial = np.stack([array,bhads_fractions_px, bhads_fractions_py, bhads_fractions_pz, bhads_projection, Secondary_Displacement], axis = -1)\n",
    "bhads_fractions_clean  = bhads_trial[(bhads_trial[:,1] < 1.5) & (bhads_trial[:,1] > 0) & (bhads_trial[:,2] < 1.5) & (bhads_trial[:,2] > 0) & (bhads_trial[:,3] < 1.5) & (bhads_trial[:,3] > 0) & (bhads_trial[:,4] > 0) & (bhads_trial[:,4] <1.5) & (bhads_trial[:,5] > 0) * (bhads_trial[:,5] < 20)]\n",
    "print(bhads_fractions_clean.shape)\n",
    "print(np.max(bhads_fractions_clean[:,1]), np.min(bhads_fractions_clean[:,1]))\n",
    "indices = bhads_fractions_clean[:,0]\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247,)\n"
     ]
    }
   ],
   "source": [
    "indices = [int(x) for x in indices]\n",
    "print(np.shape(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = tracks[indices]\n",
    "b_jets = b_jets[indices]\n",
    "bhads = bhads[indices]\n",
    "bhads_pt = bhads_pt[indices]\n",
    "bhads_eta = bhads_eta[indices]\n",
    "bhads_phi = bhads_phi[indices]\n",
    "bhads_PGID = bhads_PGID[indices]\n",
    "b_jets_m = b_jets_m[indices]\n",
    "bhads_m = bhads_m[indices]\n",
    "Secondary_Displacement = Secondary_Displacement[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247, 32, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4238/955197502.py:1: RuntimeWarning: invalid value encountered in sqrt\n",
      "  Tracks_Momentum = np.sqrt(tracks[:, : ,0]**2 + tracks[:,:,1]**2 + tracks[:,:,2]**2)\n"
     ]
    }
   ],
   "source": [
    "Tracks_Momentum = np.sqrt(tracks[:, : ,0]**2 + tracks[:,:,1]**2 + tracks[:,:,2]**2)\n",
    "Tracks_4_Momentum = np.stack([Tracks_Momentum, tracks[:,:,0], tracks[:,:,1], tracks[:,:,2]], axis = -1)\n",
    "print(Tracks_4_Momentum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247,)\n"
     ]
    }
   ],
   "source": [
    "Tracks_Invariant_Mass = np.sqrt((np.sum(Tracks_4_Momentum, axis = 1) * np.sum(Tracks_4_Momentum, axis = 1)).sum(axis = -1))\n",
    "print(Tracks_Invariant_Mass.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.9090484683110844e-06, 19.999900817871094, 1.8920154571533203)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(Secondary_Displacement), np.max(Secondary_Displacement), np.quantile(Secondary_Displacement, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247,)\n",
      "(111247,) (111247, 1)\n",
      "(111247,)\n",
      "[0.9923946  1.00824593 0.99120818]\n",
      "0.9923945992521676\n",
      "-34465.39587820514 1431.3222789813972\n",
      "0.9923945992521676\n",
      "0.9923945992521676\n",
      "0.00039231462406675546 1.4998932008025052\n",
      "Hello\n",
      "(111247,)\n",
      "(111247,)\n",
      "(111247,)\n",
      "(111247,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4238/2876407049.py:63: RuntimeWarning: divide by zero encountered in log\n",
      "  Log_Sum_px = np.log(sum_px_tracks/b_jets[:,0])\n",
      "/tmp/ipykernel_4238/2876407049.py:63: RuntimeWarning: invalid value encountered in log\n",
      "  Log_Sum_px = np.log(sum_px_tracks/b_jets[:,0])\n",
      "/tmp/ipykernel_4238/2876407049.py:64: RuntimeWarning: divide by zero encountered in log\n",
      "  Log_Sum_py = np.log(sum_py_tracks/b_jets[:,1])\n",
      "/tmp/ipykernel_4238/2876407049.py:64: RuntimeWarning: invalid value encountered in log\n",
      "  Log_Sum_py = np.log(sum_py_tracks/b_jets[:,1])\n",
      "/tmp/ipykernel_4238/2876407049.py:65: RuntimeWarning: divide by zero encountered in log\n",
      "  Log_Sum_pz = np.log(sum_pz_tracks/b_jets[:,2])\n",
      "/tmp/ipykernel_4238/2876407049.py:65: RuntimeWarning: invalid value encountered in log\n",
      "  Log_Sum_pz = np.log(sum_pz_tracks/b_jets[:,2])\n",
      "/tmp/ipykernel_4238/2876407049.py:66: RuntimeWarning: divide by zero encountered in log\n",
      "  Log_Sum_pt = np.log(sum_pt_tracks/b_jets[:,3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247,)\n",
      "(111247, 32)\n",
      "(111247, 32, 3)\n",
      "(111247, 32, 3)\n",
      "0.14894195156291137\n",
      "-671.6796543973669 468.9063272104706\n",
      "hey\n",
      "(111247, 32)\n",
      "(111247, 3)\n",
      "(111247,)\n",
      "(111247,)\n",
      "(111247,)\n",
      "79564.74\n",
      "()\n",
      "(111247,)\n",
      "(111247,)\n",
      "end\n",
      "217669.06 190874.44\n"
     ]
    }
   ],
   "source": [
    "b_jets_mag = np.linalg.norm(b_jets[:,:3], axis = 1)\n",
    "bhads_mag = np.linalg.norm(bhads[:,:3], axis = 1)\n",
    "tracks_Momentum = np.sum(np.linalg.norm(tracks[:,:,:3], axis = 2))\n",
    "\n",
    "bhads_fractions_px = bhads[:,0]/b_jets[:,0]\n",
    "bhads_fractions_py = bhads[:,1]/b_jets[:,1]\n",
    "bhads_fractions_pz = bhads[:,2]/b_jets[:,2]\n",
    "print(bhads_fractions_px.shape)\n",
    "\n",
    "b_jets_energy = np.sqrt((b_jets_m[:,0]**2) + (b_jets_mag**2))\n",
    "print(b_jets_energy.shape, b_jets_m.shape)\n",
    "\n",
    "b_jets_energy_pt = np.sqrt((b_jets_m[:,0]**2) + (b_jets[:,4]**2))\n",
    "b_jets_energy_pt.shape\n",
    "\n",
    "b_jet_energy_mass_ratio = b_jets_energy/b_jets_m[:,0]\n",
    "\n",
    "print(b_jet_energy_mass_ratio.shape)\n",
    "bhads_energy = np.sqrt((bhads_m**2) + (bhads_mag**2))\n",
    "bhads_energy.shape\n",
    "\n",
    "bhads_energy_mass_ratio = bhads_energy/bhads_m\n",
    "\n",
    "print(bhads_fractions[0])\n",
    "print(bhads[0,0]/b_jets[0,0])\n",
    "print(np.min(bhads_fractions),np.max(bhads_fractions))\n",
    "\n",
    "print(bhads_fractions_px[0])\n",
    "print(bhads[0,0]/b_jets[0,0])\n",
    "print(np.min(bhads_fractions_px),np.max(bhads_fractions_px))\n",
    "\n",
    "print(\"Hello\")\n",
    "sum_px_tracks = np.sum(tracks[:,:,0], axis = 1)\n",
    "sum_py_tracks = np.sum(tracks[:,:,1], axis = 1)\n",
    "sum_pz_tracks = np.sum(tracks[:,:,2], axis = 1)\n",
    "sum_pt_tracks = np.sum(tracks[:,:,3], axis = 1)\n",
    "print(sum_pt_tracks.shape)\n",
    "\n",
    "sum_px_tracks_RSE = np.sqrt(np.sum(tracks[:,:,0]**2, axis = 1))\n",
    "sum_py_tracks_RSE= np.sqrt(np.sum(tracks[:,:,1]**2, axis = 1))\n",
    "sum_pz_tracks_RSE = np.sqrt(np.sum(tracks[:,:,2]**2, axis = 1))\n",
    "sum_pt_tracks_RSE = np.sqrt(np.sum(tracks[:,:,3]**2, axis = 1))\n",
    "print(sum_pt_tracks_RSE.shape)\n",
    "\n",
    "RSM_scaled_px = sum_px_tracks_RSE/sum_px_tracks\n",
    "RSM_scaled_py = sum_py_tracks_RSE/sum_py_tracks\n",
    "RSM_scaled_pz = sum_pz_tracks_RSE/sum_pz_tracks\n",
    "RSM_scaled_pt = sum_pt_tracks_RSE/sum_pt_tracks\n",
    "print(RSM_scaled_pt.shape)\n",
    "\n",
    "RMS_scaled_px = np.sqrt(np.sum(tracks[:,:,0]**2, axis = 1)/MAXTRACKS)\n",
    "RMS_scaled_py = np.sqrt(np.sum(tracks[:,:,1]**2, axis = 1)/MAXTRACKS)\n",
    "RMS_scaled_pz = np.sqrt(np.sum(tracks[:,:,2]**2, axis = 1)/MAXTRACKS)\n",
    "RMS_scaled_pt = np.sqrt(np.sum(tracks[:,:,3]**2, axis = 1)/MAXTRACKS)\n",
    "print(RMS_scaled_pt.shape)\n",
    "\n",
    "Log_px_tracks = np.log(abs(tracks[:,:,0]/b_jets[:,np.newaxis,0]))\n",
    "Log_py_tracks = np.log(abs(tracks[:,:,1]/b_jets[:,np.newaxis,1]))\n",
    "Log_pz_tracks = np.log(abs(tracks[:,:,2]/b_jets[:,np.newaxis,2]))\n",
    "Log_pt_tracks = np.log(abs(tracks[:,:,3]/b_jets[:,np.newaxis,3]))\n",
    "Log_tracks = np.stack([Log_px_tracks, Log_py_tracks, Log_pz_tracks, Log_pt_tracks], axis = -1)\n",
    "\n",
    "Log_Sum_px = np.log(sum_px_tracks/b_jets[:,0])\n",
    "Log_Sum_py = np.log(sum_py_tracks/b_jets[:,1])\n",
    "Log_Sum_pz = np.log(sum_pz_tracks/b_jets[:,2])\n",
    "Log_Sum_pt = np.log(sum_pt_tracks/b_jets[:,3])\n",
    "Log_Momenta = np.log(abs(tracks_Momentum/np.sum(b_jets[:,:3], axis = 1)))\n",
    "print(Log_Sum_pt.shape)\n",
    "\n",
    "tracks_fractions_px = tracks[:,:,0]/b_jets[:,np.newaxis,0]\n",
    "tracks_fractions_py = tracks[:,:,1]/b_jets[:,np.newaxis,1]\n",
    "tracks_fractions_pz = tracks[:,:,2]/b_jets[:,np.newaxis,2]\n",
    "tracks_fractions_pt = tracks[:,:,3]/b_jets[:,np.newaxis,3]\n",
    "print(tracks_fractions_pt.shape)\n",
    "Track_fractions = np.stack([tracks_fractions_px,tracks_fractions_py, tracks_fractions_pz], axis = -1)\n",
    "print(Track_fractions.shape)\n",
    "\n",
    "print(Track_fractions.shape)\n",
    "print(tracks[0,0,0]/b_jets[0,0])\n",
    "print(np.mean(Track_fractions),np.std(Track_fractions))\n",
    "\n",
    "print(\"hey\")\n",
    "Tracks_projection = ((tracks[:,:,:3]*b_jets[:,np.newaxis,:3]).sum(axis = 2)/(b_jets_mag[:,np.newaxis]**2))\n",
    "print(Tracks_projection.shape)\n",
    "Track_Momenta = np.stack([sum_px_tracks, sum_py_tracks, sum_pz_tracks], axis = -1)\n",
    "print(Track_Momenta.shape)\n",
    "Sum_Tracks_projection = ((Track_Momenta*b_jets[:,:3]).sum(axis = 1))/(b_jets_mag**2)\n",
    "print(Sum_Tracks_projection.shape)\n",
    "\n",
    "b_jet_energy_ratio_px = sum_px_tracks/b_jets_energy\n",
    "b_jet_energy_ratio_py = sum_py_tracks/b_jets_energy\n",
    "b_jet_energy_ratio_pz = sum_pz_tracks/b_jets_energy\n",
    "b_jet_energy_ratio_pt = sum_pt_tracks/b_jets_energy\n",
    "print(b_jet_energy_ratio_pt.shape)\n",
    "\n",
    "b_jet_energy_ratio_cart = b_jets_mag/b_jets_energy\n",
    "b_jet_energy_ratio_pt = b_jets[:,4]/b_jets_energy\n",
    "print(b_jet_energy_ratio_pt.shape)\n",
    "\n",
    "b_jet_energy_ratio_total = np.sum(b_jets[:,4])/np.sum(b_jets_energy)  \n",
    "b_jet_transverse_mass = np.sqrt(b_jets_energy**2 - b_jets[:,2]**2)\n",
    "bhads_transverse_mass = np.sqrt(bhads_energy**2 - bhads[:,2]**2)\n",
    "print(b_jet_transverse_mass[0])\n",
    "print(b_jet_energy_ratio_total.shape)\n",
    "print(b_jet_transverse_mass.shape)\n",
    "print(np.full((len(b_jets)),b_jet_energy_ratio_total).shape)\n",
    "print(\"end\")\n",
    "bhads_projection = ((bhads[:,:3]*b_jets[:,:3]).sum(axis = 1))/(b_jets_mag**2)\n",
    "\n",
    "print(np.mean(b_jets_energy),np.std(b_jets_energy))\n",
    "\n",
    "b_jets = np.stack([b_jets[:,0], b_jets[:,1], b_jets[:,2],b_jets[:,3],b_jets[:,4], b_jets[:,5], b_jets[:,6], b_jets_mag, sum_px_tracks, sum_py_tracks, sum_pz_tracks, sum_pt_tracks, sum_px_tracks_RSE, sum_py_tracks_RSE, sum_pz_tracks_RSE, sum_pt_tracks_RSE, RSM_scaled_px, RSM_scaled_py, RSM_scaled_pz, RSM_scaled_pt, RMS_scaled_px, RMS_scaled_py, RMS_scaled_pz, RMS_scaled_pt, b_jet_transverse_mass, Log_Sum_px, Log_Sum_py, Log_Sum_pz, Log_Sum_pt, Log_Momenta, b_jets_energy, b_jet_energy_ratio_px, b_jet_energy_ratio_py, b_jet_energy_ratio_pz, b_jet_energy_ratio_cart, b_jet_energy_ratio_pt, b_jet_energy_mass_ratio, np.full((len(b_jets),),b_jet_energy_ratio_total)], axis = -1)\n",
    "bhads_targets = np.stack([bhads[:,0],bhads[:,1], bhads[:,2], bhads_pt, bhads_eta, bhads_phi, bhads_fractions_px, bhads_fractions_py, bhads_fractions_pz, bhads_energy, bhads_transverse_mass, bhads_energy_mass_ratio, bhads_projection, bhads_m, Secondary_Displacement], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247, 32, 18)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "0 - Momentum Px Scaled\n",
    "1 - Momentum Py Scaled\n",
    "2 - Momentum Pz Scaled\n",
    "3-  Momentum Pt Scaled\n",
    "4-  Momentum Eta Scaled\n",
    "5 - Momentum Phi Scaled\n",
    "6 - Tranvserse Impact parameter sin component\n",
    "7 - Longitudinal Impact parameter\n",
    "8 - Longitudinal Impact parameter signficiance\n",
    "9 - Longitudinal Impact Parameter w.r.t PV\n",
    "10 - Longitudinal Impact Parameter wr.r.t PV\n",
    "11 - Momentum Fraction Px Scaled\n",
    "12 - Momentum Fraction Py Scaled\n",
    "13 - Momentum Fraction Pz Scaled\n",
    "14 - Momentum Fraction pt Scaled.\n",
    "15 - Logarithm of px of the tracks / b_jet x momenta\n",
    "16 - Logarithm of py of the tracks / b_jet y momenta\n",
    "17 - Logarithm of pz of the tracks / b_jet z momenta\n",
    "18 - Logarithm of sum pt of the tracks / b_jet t momenta\n",
    "\"\"\"\n",
    "Tracks_input = np.concatenate([tracks, Track_fractions, Log_tracks], axis = -1)\n",
    "print(Tracks_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247, 71)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "0 - Momentum Px Scaled\n",
    "1 - Momentum Py Scaled\n",
    "2 - Momentum Pz Scaled\n",
    "3 - Momentum Pt Scaled\n",
    "4 - Momentum eta Scaled\n",
    "5 - Momentum Phi Scaled\n",
    "6 - Sum px of the tracks\n",
    "7 - Sum py of the tracks\n",
    "8 - Sum pz of the tracks\n",
    "9 - Sum pt of the tracks\n",
    "10 - Sqrt of the Sum px of the tracks\n",
    "11 - Sqrt of the Sum py of the tracks\n",
    "12 - Sqrt of the Sum pz of the tracks\n",
    "13 - Sqrt of the Sum pt of the tracks\n",
    "14 - Sqrt of the Sum px of the tracks scaled by the sum px of the tracks\n",
    "15 - Sqrt of the Sum py of the tracks scaled by the sum py of the tracks\n",
    "16 - Sqrt of the Sum pz of the tracks scaled by the sum pz of the tracks\n",
    "17 - Sqrt of the Sum pt of the tracks scaked by the sum pt of the tracks\n",
    "18 - Root Mean Square of the px momenta of the tracks\n",
    "19 - Root Mean Square of the py momenta of the tracks\n",
    "20 - Root Mean Square of the pz momenta of the tracks\n",
    "21 - Root Mean Square of the pt momenta of the tracks\n",
    "22 - Tranvserse mass of the b-jets\n",
    "23 - Logarithm of the  Sum px of the tracks divide by the b_jet x momenta\n",
    "24 - Logarithm of the  Sum py of the tracks divide by the b_jet y momenta\n",
    "25 - Logarithm of the  Sum pz of the tracks divide by the b_jet z momenta\n",
    "26 - Logarithm of the total momenta of the tracks divided by the b_jet total momenta\n",
    "27 - B_jet energy\n",
    "28 - B_jet energy ratio px\n",
    "29 - B_jet energy ratio py\n",
    "30 - B_jet energy ratio pz\n",
    "31 - B_jet energy ratio pt\n",
    "32 - B_jet energy ratio cart\n",
    "32 - B_jet energy ratio pt\n",
    "33 - B_jet energy ratio total\n",
    "\"\"\"\n",
    "b_jets_input = np.concatenate([b_jets, Tracks_projection, Sum_Tracks_projection.reshape(-1,1)], axis = -1)\n",
    "print(b_jets_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-521: 24469, 521: 24267, -511: 22928, 511: 22873, 531: 5519, -531: 5472, 5122: 2263, -5122: 2220, -5132: 317, 5132: 304, 5232: 298, -5232: 297, 5332: 8, -5332: 7, -541: 3, 555: 1, 541: 1})\n",
      "(111247, 1)\n",
      "Counter({0.0: 48736, 1.0: 45801, 2.0: 10991, 4.0: 5719})\n",
      "(111247, 4)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(bhads_PGID)\n",
    "print(counter)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "counter = np.array([])\n",
    "bhadron_PGIDs = []\n",
    "for PGID in bhads_PGID:\n",
    "    if (PGID == 521 or PGID == -521):\n",
    "        element = 0\n",
    "    elif (PGID == 511 or PGID == -511):\n",
    "        element = 1\n",
    "    elif (PGID == 531 or PGID == -531):\n",
    "        element = 2\n",
    "    else:\n",
    "        element = 4\n",
    "    counter = np.append(counter, [element])\n",
    "    bhadron_PGIDs.append([str(element)])\n",
    "print(np.shape(bhadron_PGIDs))\n",
    "counter = Counter(counter)\n",
    "print(counter)\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "onehot = encoder.fit_transform(bhadron_PGIDs)\n",
    "print(np.shape(onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247, 71)\n",
      "(111247, 32, 18) (111247, 71) (111247,) (111247, 15)\n"
     ]
    }
   ],
   "source": [
    "mask = np.where(np.isinf(b_jets_input) == True)\n",
    "b_jets_input_clean = np.delete(b_jets_input, mask, axis = 0)\n",
    "print(b_jets_input_clean.shape)\n",
    "Tracks_input_clean = np.delete(Tracks_input, mask, axis = 0)\n",
    "bhads_m_clean = np.delete(bhads_m, mask, axis = 0)\n",
    "bhads_targets_clean = np.delete(bhads_targets, mask, axis = 0)\n",
    "onehot = np.delete(onehot, mask, axis = 0)  \n",
    "print(Tracks_input_clean.shape,b_jets_input_clean.shape,bhads_m_clean.shape, bhads_targets_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111247, 32, 18)\n",
      "(111247, 71)\n",
      "(111247, 15)\n",
      "[-1.48469600e+01 -2.92342189e+02 -3.23874834e+02  8.24367735e+04\n",
      " -2.23476102e-03 -4.95291308e-03  7.80586088e-01  7.80190123e-01\n",
      "  7.76651797e-01  1.64101845e+05  8.26581677e+04  3.09222926e+01\n",
      "  7.77563556e-01  5.30749206e+03  1.64801865e+00] [6.53973176e+04 6.51703717e+04 2.01842188e+05 4.15722329e+04\n",
      " 1.23665467e+00 1.81252921e+00 2.27860762e-01 2.28764476e-01\n",
      " 2.26390537e-01 1.49543331e+05 4.14713969e+04 2.81743538e+01\n",
      " 2.18175528e-01 8.82691460e+01 2.68572573e+00]\n"
     ]
    }
   ],
   "source": [
    "Scaler_tracks = StandardScaler()\n",
    "Num_events,Num_tracks,Num_features = np.shape(Tracks_input_clean)\n",
    "Scaled_tracks = np.reshape(Tracks_input_clean, newshape=(-1,Num_features))\n",
    "tracks_scaled = Scaler_tracks.fit_transform(Scaled_tracks)\n",
    "Tracks_input_scaled = np.reshape(tracks_scaled, newshape= (Num_events,Num_tracks,Num_features))\n",
    "print(np.shape(Tracks_input_scaled))\n",
    "\n",
    "Scaler_jets = StandardScaler()\n",
    "Num_events,Num_features = np.shape(b_jets_input_clean)\n",
    "b_jets_scaled = np.reshape(b_jets_input_clean, newshape=(-1,Num_features))\n",
    "b_jets_scaled = Scaler_jets.fit_transform(b_jets_scaled)\n",
    "b_jets_input_scaled = np.reshape(b_jets_scaled, newshape= (Num_events,Num_features))\n",
    "print(np.shape(b_jets_input_scaled))\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "lister = []\n",
    "for bhads_target_feature in range(np.shape(bhads_targets_clean)[1]):\n",
    "    Bhads_targets = bhads_targets_clean[:,bhads_target_feature]\n",
    "    mean, std = np.mean(Bhads_targets), np.std(Bhads_targets)\n",
    "    means = np.append(means,mean)\n",
    "    stds = np.append(stds,std)\n",
    "    Standardized_Bhads_targets = (Bhads_targets - mean)/(std)\n",
    "    Standardized_Bhads_targets = Standardized_Bhads_targets.reshape(-1,1)\n",
    "    lister.append(Standardized_Bhads_targets)\n",
    "Standardized_Bhads_targets = np.concatenate(lister,axis = 1)\n",
    "print(Standardized_Bhads_targets.shape)\n",
    "print(means,stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs = {}):\n",
    "        self.logs = []\n",
    "    def on_epoch_begin(self, epoch, logs ={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        self.logs.append(timer() - self.starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "d_model = 64\n",
    "dff = 64\n",
    "num_heads = 4\n",
    "n_features = np.shape(b_jets_input_scaled)[1]\n",
    "n_targets = np.shape(Standardized_Bhads_targets)[1]\n",
    "n_targets_classification = np.shape(onehot)[1]\n",
    "dropout_rate = 0.01\n",
    "track_layers = 4\n",
    "jet_layers = [256,256,256,256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 16:19:30.179557: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-15 16:19:30.179595: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-15 16:19:30.179619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vonneumann.csc.warwick.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2023-02-15 16:19:30.180327: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ParticleTranformer = hffragT.hffragTransformer(\n",
    "    track_layers = track_layers,\n",
    "    n_targets_classifications = n_targets_classification,\n",
    "    jet_layers =  jet_layers, \n",
    "    d_model = d_model, \n",
    "    num_heads = num_heads,\n",
    "    MASKVAL= MASKVAL,\n",
    "    dff = dff,\n",
    "    n_targets= n_targets,\n",
    "    n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rating = TransformerSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Nadam(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepSetNeuralNetArchitecture import LogNormal_Loss_Function\n",
    "from HffragDeepSetsProjection import Multivariate_Gaussian_Negative_Likelihood_Loss_Curve\n",
    "ParticleTranformer.compile(\n",
    " optimizer = optimizer,\n",
    " loss = {\"output_1\":Multivariate_Gaussian_Negative_Likelihood_Loss_Curve, \"output_2\":tf.keras.losses.categorical_crossentropy},\n",
    " metrics = [Mean_Squared_Error]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(111247, 135), dtype=float32, numpy=\n",
       " array([[-1.8061552 ,  0.41888148, -0.30458838, ...,  0.9929388 ,\n",
       "         -2.117968  ,  0.73637986],\n",
       "        [-1.4064045 ,  0.15506133,  0.5014766 , ...,  1.4308343 ,\n",
       "         -2.2362132 ,  0.9922315 ],\n",
       "        [-1.8673229 , -0.15039937,  0.19276933, ...,  0.6320064 ,\n",
       "         -2.0026073 ,  0.6728825 ],\n",
       "        ...,\n",
       "        [-1.3636396 ,  1.042592  , -0.37975532, ...,  1.6399451 ,\n",
       "         -2.1410525 ,  0.45532534],\n",
       "        [-1.8827037 ,  0.033855  ,  0.0770864 , ...,  0.69733286,\n",
       "         -1.9565624 ,  0.62951297],\n",
       "        [-1.7601204 ,  0.30267957,  0.0713824 , ...,  0.8211922 ,\n",
       "         -2.0237122 ,  0.7445887 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(111247, 4), dtype=float32, numpy=\n",
       " array([[0.2495561 , 0.24963978, 0.25096476, 0.24983928],\n",
       "        [0.24955562, 0.24964114, 0.2509646 , 0.24983858],\n",
       "        [0.24955602, 0.24964015, 0.2509654 , 0.24983853],\n",
       "        ...,\n",
       "        [0.2495564 , 0.24963982, 0.25096422, 0.24983954],\n",
       "        [0.24955575, 0.24964026, 0.25096545, 0.24983853],\n",
       "        [0.2495561 , 0.2496398 , 0.2509652 , 0.24983896]], dtype=float32)>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = ParticleTranformer((Tracks_input_scaled,b_jets_input_scaled))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hffrag_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  334912    \n",
      "                                                                 \n",
      " b__jet__layer (B_Jet_Layer)  multiple                 141209    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 476,121\n",
      "Trainable params: 476,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ParticleTranformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce early_stopping to prevent overfitting\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.00001,  # The minimum amount of change to count as an improvement\n",
    "    patience=50,  # The number of epochs to wait before stopping\n",
    "    restore_best_weights=True,  # Keep the best weights\n",
    ")\n",
    "# Prevent spikes in the validation and training loss due to the gradient descent kicking the network out of a local minima\n",
    "reduce_learn_on_plateau = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.80, patience=15, min_lr=1e-9)\n",
    "\n",
    "# Save the weights of the model to allow reuse in future.\n",
    "path = \"/home/physics/phujdj/DeepLearningParticlePhysics/CheckPointsDeepNet/TransformerWeights&Biases.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                 save_weights_only=True, verbose=0, save_freq = 100*BATCHSIZE)\n",
    "#Timer\n",
    "#cb = TimingCallback()\n",
    "\n",
    "#Weight&Biases Callback:\n",
    "#Wanda = WandbCallback(save_graph = True,save_weights_only = True, log_weights = True, log_gradients = True, log_evaluation = True, training_data = (X_train,y_train), validation_data = (X_valid,y_valid), log_batch_frequency = 5)\n",
    "\n",
    "# Learning Scheduler:\n",
    "exponential_decay_fn = DSNNA.expontial_decay(lr0 = LR,s = 100)\n",
    "learning_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From /home/physics/phujdj/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/physics/phujdj/anaconda3/envs/SpocFit/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "1214/1217 [============================>.] - ETA: 0s - loss: 21.5052 - output_1_loss: 15.3363 - output_2_loss: 1.3163 - output_1_Mean_Squared_Error: 0.7674 - output_2_Mean_Squared_Error: 0.1788"
     ]
    }
   ],
   "source": [
    "history  = ParticleTranformer.fit(\n",
    "    (Tracks_input_scaled,b_jets_input_scaled), y = {\"output_1\":Standardized_Bhads_targets, \"output_2\":onehot},\n",
    "    validation_split = 0.3,\n",
    "    epochs = EPOCHS,\n",
    "    batch_size = BATCHSIZE,\n",
    "    callbacks = [early_stopping, cp_callback, reduce_learn_on_plateau],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a05ddcd8ffea9a6a7d2e914b733df5445b717626b5b8c92c04bfc4eb6e7f5cba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
