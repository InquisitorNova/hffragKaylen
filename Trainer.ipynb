{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras import callbacks\n",
    "import keras\n",
    "import DeepSetNeuralNetArchitecture as DSNNA\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import sklearn as sk\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "from hffrag import fixedbinning\n",
    "from hffrag import binneddensity\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is being stored in a tree datastructure.\n",
    "# We access the charm root using this command\n",
    "tree = uproot.open(\"hffrag.root:CharmAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "MASKVAL = -999 # This value is introduced to ensure arrays are regular (Of the same size). They will be masked later by the network\n",
    "MAXTRACKS = 32 # This value is the maximum number of tracks allowed per event\n",
    "BATCHSIZE = 64 # This is the batch size of the mini batches used during training\n",
    "EPOCHS = 10000 # This is the default number of epochs for which the neural network will train providing that early stopping does not occur\n",
    "MAXEVENTS = 1e20 #This is the maximum number of events that will the program will accept\n",
    "LR = 1e-4 #This is the default learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we wish to study\n",
    "track_features = [\"AnalysisTracks_pt\", \"AnalysisTracks_eta\", \"AnalysisTracks_phi\", \"AnalysisTracks_z0sinTheta\",\n",
    "                  \"AnalysisTracks_d0sig\", \"AnalysisTracks_d0\", \"AnalysisTracks_d0sigPV\", \"AnalysisTracks_d0PV\"]\n",
    "jet_features = [\"AnalysisAntiKt4TruthJets_pt\", \"AnalysisAntiKt4TruthJets_eta\", \"AnalysisAntiKt4TruthJets_phi\",\n",
    "                \"AnalysisAntiKt4TruthJets_ghostB_pt\", \"AnalysisAntiKt4TruthJets_ghostB_eta\",\"AnalysisAntiKt4TruthJets_ghostB_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the root file\n",
    "features = tree.arrays(jet_features+track_features, entry_stop=MAXEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the events of interest\n",
    "events = features[ak.sum(\n",
    "    features[\"AnalysisAntiKt4TruthJets_pt\"] > 25000, axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the number of jets being trained on\n",
    "jets = events[jet_features][:, 0]\n",
    "print(\"The number of jets to train on is: \", len(jets))\n",
    "print(\"The number of track features is: \",len(track_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tracks from the events\n",
    "tracks = events[track_features]\n",
    "\n",
    "# Match the tracks to the jets\n",
    "matchedtracks = tracks[DSNNA.Match_Tracks(jets, tracks)]\n",
    "\n",
    "# Pad and Flatten the data\n",
    "matchedtracks = DSNNA.flatten(matchedtracks, MAXTRACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the the bottom jets and their associated tracks\n",
    "bjets = ak.sum(jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] > 5000, axis=1) > 0\n",
    "jets = jets[bjets]\n",
    "\n",
    "# Obtain the pt, eta and phi of each b hadron jet\n",
    "bhads_pt = jets[\"AnalysisAntiKt4TruthJets_ghostB_pt\"][:, 0].to_numpy()\n",
    "bhads_eta = jets[\"AnalysisAntiKt4TruthJets_ghostB_eta\"][:,0].to_numpy()\n",
    "bhads_phi = jets[\"AnalysisAntiKt4TruthJets_ghostB_phi\"][:,0].to_numpy()\n",
    "\n",
    "bhads = np.stack([bhads_pt,bhads_eta,bhads_phi],axis = -1) #Combine the momentum, eta and phi for each jet into one array\n",
    "\n",
    "print(\"There are {} outputs\".format(np.shape(bhads)[1])) # Display the number of target features the neural network will predict\n",
    "matchedtracks = matchedtracks[bjets]\n",
    "print(\"There are {} inputs\".format(np.shape(matchedtracks)[1])) # Display the number of target features the neural network will use in it's ppredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(bhads)) #Check the shape of the neural network\n",
    "print(np.shape(jet_features[:-1])) #Check for shape of the jet features\n",
    "print(jets[jet_features[0]]) # Check the jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the jet and tracks to unstructed data.\n",
    "jets = structured_to_unstructured(jets[jet_features[:-3]])\n",
    "matchedtracks = structured_to_unstructured(matchedtracks)\n",
    "print(np.shape(jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the matchtracks are the correct shape\n",
    "print(matchedtracks[:, 0:1])\n",
    "print(np.shape(matchedtracks[:, :, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the coordinates of the b jets and tracks to cartesian coordinates\n",
    "tracks_p = DSNNA.pt_eta_phi_2_px_py_pz_tracks(matchedtracks.to_numpy())\n",
    "bhads = DSNNA.pt_eta_phi_2_px_py_pz_jets(bhads)\n",
    "\n",
    "#Check the shape of the momenta of the tracks and the rest of the data is consistent\n",
    "print(np.shape(tracks_p))\n",
    "print(np.shape(matchedtracks[:, :, 3:]))\n",
    "\n",
    "#Combine the momenta of the tracks with the rest of the track features to form the track dataset\n",
    "tracks = np.concatenate([tracks_p,matchedtracks[:,:,3:].to_numpy()],axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that this is all the correct shape\n",
    "print(np.shape(tracks))\n",
    "print(np.shape(bhads))\n",
    "print(tracks[0,0])\n",
    "print(bhads[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    tracks, bhads, train_size=0.7, random_state=42)\n",
    "#Save the training and validation datasets.\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/X_train_data.npy\",X_train)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/X_valid_data.npy\",X_valid)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/y_train_data.npy\",y_train)\n",
    "np.save(\"/home/physics/phujdj/DeepLearningParticlePhysics/TrainingAndValidationData/y_valid_data.npy\",y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cyclical Learning Rate Scheduler:\n",
    "steps_per_epoch = len(X_train)\n",
    "clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate = 1e-4,\n",
    "maximal_learning_rate = 1e-3,\n",
    "scale_fn = lambda x: 0.90 ** x,\n",
    "step_size = 2.0 * steps_per_epoch\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "\n",
    "# Builds the deep neural network\n",
    "track_layers = [32,32,32,32,32]\n",
    "jet_layers = [64,64,64,64,64]\n",
    "DeepNet = DSNNA.DeepSetNeuralNetwork(\n",
    "    [len(track_features)]+track_layers, jet_layers,3,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarises the Neural Network Architecture\n",
    "DeepNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(DeepNet, to_file =\"NetworkArchitecture.png\", show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for the of the training and validation sets\n",
    "print(np.shape(X_train), np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs = {}):\n",
    "        self.logs = []\n",
    "    def on_epoch_begin(self, epoch, logs ={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        self.logs.append(timer() - self.starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce early_stopping to prevent overfitting\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.00001,  # The minimum amount of change to count as an improvement\n",
    "    patience=30,  # The number of epochs to wait before stopping\n",
    "    restore_best_weights=True,  # Keep the best weights\n",
    ")\n",
    "# Prevent spikes in the validation and training loss due to the gradient descent kicking the network out of a local minima\n",
    "reduce_learn_on_plateau = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "\n",
    "# Save the weights of the model to allow reuse in future.\n",
    "path = \"/home/physics/phujdj/DeepLearningParticlePhysics/CheckPoints/DeepNetWeights&Biases.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n",
    "                                                 save_weights_only=True, verbose=0, save_freq = 100 )\n",
    "#Timer\n",
    "cb = TimingCallback()\n",
    "\n",
    "# Learning Scheduler:\n",
    "exponential_decay_fn = DSNNA.expontial_decay(lr0 = LR,s = 30)\n",
    "learning_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0,EPOCHS * steps_per_epoch)\n",
    "lr = clr(steps)\n",
    "plt.plot(steps,lr)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "history = DeepNet.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=BATCHSIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[cp_callback,cb,reduce_learn_on_plateau]  # Enter call back\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and validation curves vs epoch\n",
    "history_df = pd.DataFrame(history.history)\n",
    "np.log(history_df.loc[:, [\"loss\", \"val_loss\"]]).plot()\n",
    "history.to_csv('/home/physics/DeepLearningParticlePhysics/history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(cb.logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to the console the minimum epoch\n",
    "print(\"Minimum validation loss: {}\".format(history_df[\"val_loss\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the entire performance of the model\n",
    "loss = DeepNet.evaluate(tracks,bhads,verbose = 2)\n",
    "print(\"The Loaded DeepNet has loss: \", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('SpocFit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a05ddcd8ffea9a6a7d2e914b733df5445b717626b5b8c92c04bfc4eb6e7f5cba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
